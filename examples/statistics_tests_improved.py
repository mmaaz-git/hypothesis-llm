"""Property-based tests for statistics module.
Generated by hypothesis-llm.
"""

import hypothesis
from hypothesis import given, strategies as st
from statistics import (
    mean,
    median,
    mode,
    stdev,
    variance
)




@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))
def test_mean_bounded_by_min_max(data):
    """Test that min(data) ≤ mean(data) ≤ max(data)"""
    result = mean(data)
    assert min(data) <= result <= max(data)



@given(st.floats(allow_nan=False, allow_infinity=False))
def test_mean_single_element_invariant(x):
    """Test that mean([x]) == x for any single element"""
    import math
    result = mean([x])
    assert math.isclose(result, x, rel_tol=1e-9)



@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))
def test_mean_order_independence(data):
    """Test that mean(data) == mean(shuffled(data))"""
    import math
    import random
    
    original_mean = mean(data)
    shuffled_data = data.copy()
    random.shuffle(shuffled_data)
    shuffled_mean = mean(shuffled_data)
    
    assert math.isclose(original_mean, shuffled_mean, rel_tol=1e-9)


@given(st.lists(st.integers(), min_size=1))
def test_bounded_output(data):
    """Test that median is bounded by min and max of data."""
    result = median(data)
    assert min(data) <= result <= max(data)


@given(st.integers())
def test_single_element_identity(x):
    """Test that median of single element returns that element."""
    assert median([x]) == x



@given(st.integers())
def test_single_element_data_returns_that_element(element):
    """Test that single element data returns that element."""
    data = [element]
    result = mode(data)
    assert result == element


@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1000, max_value=1000))
def test_stdev_single_duplication_invariance(x):
    """Test that stdev([x, x]) == 0."""
    import math
    data = [x, x]
    result = stdev(data)
    assert math.isclose(result, 0, abs_tol=1e-15)


@given(st.floats(allow_nan=False, allow_infinity=False), st.integers(min_value=2, max_value=20))
def test_variance_zero_for_constant_data(constant_value, size):
    """Test that variance is zero when all data points are identical."""
    import math
    data = [constant_value] * size
    result = variance(data)
    assert math.isclose(result, 0, abs_tol=1e-10)



@given(st.lists(st.integers(min_value=-1000, max_value=1000), min_size=2, max_size=50))
def test_stdev_equals_sqrt_of_variance_integers(data):
    """Test that stdev(data) == sqrt(variance(data)) for integer data.
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance with integer inputs.
    """
    import math
    
    stdev_result = stdev(data)
    variance_result = variance(data)
    sqrt_variance = math.sqrt(variance_result)
    
    assert math.isclose(stdev_result, sqrt_variance, rel_tol=1e-12)



@given(st.lists(st.fractions(min_value=-100, max_value=100), min_size=2, max_size=20))
def test_variance_with_explicit_mean_equals_variance_with_none_fractions(data):
    """
    Test the same property with Fraction data for exact arithmetic.
    
    Using Fraction objects ensures exact arithmetic without floating point
    precision issues, allowing us to test for exact equality.
    """
    from fractions import Fraction
    
    # Calculate the mean of the data
    computed_mean = mean(data)
    
    # Get variance with explicit mean
    variance_with_mean = variance(data, computed_mean)
    
    # Get variance with None (auto-calculated mean)
    variance_with_none = variance(data, None)
    
    # They should be exactly equal for Fraction inputs
    assert variance_with_mean == variance_with_none



@given(st.lists(st.fractions(), min_size=2, max_size=20))
def test_stdev_with_explicit_mean_equals_stdev_with_none_fractions(data):
    """
    Test that stdev(data, mean(data)) == stdev(data, None) for Fraction data.
    
    This tests the same property but with Fraction input data to ensure
    the equivalence holds for exact rational arithmetic.
    """
    from statistics import stdev, mean
    
    # Calculate stdev with explicit mean
    data_mean = mean(data)
    stdev_with_mean = stdev(data, data_mean)
    
    # Calculate stdev with None (let it calculate mean internally)
    stdev_with_none = stdev(data, None)
    
    # For Fraction data, the results should be exactly equal
    assert stdev_with_mean == stdev_with_none, \
        f"stdev with explicit mean {stdev_with_mean} != stdev with None {stdev_with_none}"

@given(st.just([]))
def test_mean_empty_list_raises_error(data):
    """Test that mean([]) raises StatisticsError"""
    import pytest
    from statistics import mean, StatisticsError
    with pytest.raises(StatisticsError):
        mean(data)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1), 
       st.floats(allow_nan=False, allow_infinity=False, min_value=-1e6, max_value=1e6))
def test_mean_linear_scaling(data, k):
    """Test that mean([k*x for x in data]) == k * mean(data)
    
    This property tests the linearity of the mean function under scalar multiplication.
    Special handling is needed for k=0 due to floating-point representation issues,
    and adaptive tolerance is used to account for accumulated precision errors.
    """
    import math
    
    scaled_data = [k * x for x in data]
    original_mean = mean(data)
    scaled_mean = mean(scaled_data)
    expected = k * original_mean
    
    # Handle the special case where k=0
    # When k=0, all scaled_data elements are 0, so mean should be exactly 0.0
    if k == 0:
        assert scaled_mean == 0.0
    else:
        # Use adaptive tolerance based on the magnitude of values involved
        # This accounts for accumulated floating-point errors in both approaches
        max_abs_value = max(abs(original_mean), abs(scaled_mean), abs(expected))
        
        # Set relative tolerance based on the scale of the problem
        # For very small values, we need a larger relative tolerance
        # For normal values, use a reasonable relative tolerance
        rel_tolerance = max(1e-12, 1e-14 * max_abs_value) if max_abs_value > 1e-6 else 1e-9
        
        # Use both relative and absolute tolerance for robustness
        assert math.isclose(scaled_mean, expected, rel_tol=rel_tolerance, abs_tol=1e-15), \
            f"scaled_mean={scaled_mean}, expected={expected}, diff={abs(scaled_mean - expected)}"

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1),
       st.floats(allow_nan=False, allow_infinity=False, min_value=-1e3, max_value=1e3))
def test_mean_translation_invariant(data, c):
    """Test that mean([x+c for x in data]) == mean(data) + c
    
    This property tests translation invariance: adding a constant to all values
    should shift the mean by that same constant. Uses a reduced range for c
    and appropriate tolerance to handle floating-point precision issues.
    """
    import math
    translated_data = [x + c for x in data]
    original_mean = mean(data)
    translated_mean = mean(translated_data)
    expected = original_mean + c
    assert math.isclose(translated_mean, expected, rel_tol=1e-12, abs_tol=1e-12)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, max_value=1e100, min_value=-1e100), min_size=1),
       st.lists(st.floats(allow_nan=False, allow_infinity=False, max_value=1e100, min_value=-1e100), min_size=1))
def test_mean_concatenation_property(a, b):
    """
    Test that mean(a + b) == (len(a)*mean(a) + len(b)*mean(b)) / (len(a) + len(b))
    
    This property tests the linearity of the mean function over concatenated lists.
    The mean of concatenated lists should equal the weighted average of the individual means,
    where weights are the lengths of the respective lists.
    
    We constrain float values to avoid extreme numbers that cause precision issues,
    and use a relaxed tolerance to account for floating point arithmetic limitations.
    """
    import math
    concatenated = a + b
    concatenated_mean = mean(concatenated)
    
    mean_a = mean(a)
    mean_b = mean(b)
    expected = (len(a) * mean_a + len(b) * mean_b) / (len(a) + len(b))
    
    # Use relaxed tolerance to handle floating point precision issues
    # especially when dealing with numbers of different magnitudes
    assert math.isclose(concatenated_mean, expected, rel_tol=1e-6, abs_tol=1e-9)

import math

@given(st.lists(st.integers(), min_size=1))
def test_mean_integer_input(data):
    """Test that mean of integers returns int or float and is mathematically correct"""
    result = mean(data)
    assert isinstance(result, (int, float))
    # Verify correctness
    expected = sum(data) / len(data)
    assert result == expected

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))
def test_mean_float_input(data):
    """Test that mean of floats returns float and is mathematically correct"""
    result = mean(data)
    assert isinstance(result, float)
    # Verify correctness with tolerance for floating point arithmetic
    expected = sum(data) / len(data)
    assert math.isclose(result, expected, rel_tol=1e-9)

@given(st.lists(st.one_of(st.integers(), st.floats(allow_nan=False, allow_infinity=False)), min_size=1))
def test_mean_mixed_type_input(data):
    """Test that mean of mixed int/float data returns correct result"""
    result = mean(data)
    assert isinstance(result, (int, float))
    # Verify correctness with tolerance for floating point arithmetic
    expected = sum(data) / len(data)
    if isinstance(expected, float):
        assert math.isclose(result, expected, rel_tol=1e-9)
    else:
        assert result == expected

@given(st.floats(allow_nan=False, allow_infinity=False),
       st.integers(min_value=1, max_value=100))
def test_mean_duplicate_invariant(x, n):
    """Test that mean([x]*n) == x
    
    This property tests the invariant that the mean of n identical values
    should equal the original value. We use both relative and absolute
    tolerances to handle floating-point precision issues:
    - Relative tolerance handles larger values appropriately
    - Absolute tolerance handles cases where x is very small or zero
    """
    import math
    data = [x] * n
    result = mean(data)
    assert math.isclose(result, x, rel_tol=1e-9, abs_tol=1e-15)

@given(st.lists(st.fractions(), min_size=1))
def test_mean_fraction_type_preservation(data):
    """Test that mean preserves Fraction type and computes correct mathematical mean"""
    from fractions import Fraction
    
    result = mean(data)
    
    # Test type preservation: result should be a Fraction
    assert isinstance(result, Fraction)
    
    # Test mathematical correctness: verify the mean is calculated correctly
    expected = sum(data) / len(data)
    assert result == expected

@given(st.lists(st.decimals(allow_nan=False, allow_infinity=False, min_value=-1000, max_value=1000, places=2), min_size=1, max_size=100))
def test_mean_decimal_type_preservation(data):
    """Test that mean preserves Decimal type and computes correct value
    
    This test verifies two key properties:
    1. Type preservation: The result should be a Decimal when input is Decimal
    2. Correctness: The computed mean should equal the mathematical mean
    
    The strategy uses bounded decimals to avoid overflow issues and extreme
    precision problems that could affect the calculation.
    """
    from decimal import Decimal
    
    result = mean(data)
    
    # Test type preservation - result should be Decimal type
    assert isinstance(result, Decimal), f"Expected Decimal, got {type(result)}"
    
    # Test correctness of the mean calculation
    # Use Decimal arithmetic throughout to maintain precision
    expected_mean = sum(data, Decimal('0')) / Decimal(len(data))
    assert result == expected_mean, f"Mean calculation incorrect: got {result}, expected {expected_mean}"

@given(st.lists(st.integers(), min_size=1))
def test_order_invariance(data):
    """Test that median is invariant to the order of input data.
    
    The median of a dataset should be the same regardless of how the
    elements are ordered. This test verifies that the median function
    produces identical results for the original data, sorted data,
    reverse-sorted data, and randomly shuffled data.
    """
    import random
    
    # Calculate median of original data as the reference
    original_median = median(data)
    
    # Test with sorted data - should give same result
    sorted_data = sorted(data)
    assert median(sorted_data) == original_median, "Median should be invariant to sorting"
    
    # Test with reverse sorted data - should give same result
    reverse_sorted_data = sorted(data, reverse=True)
    assert median(reverse_sorted_data) == original_median, "Median should be invariant to reverse sorting"
    
    # Test with shuffled data - should give same result
    shuffled_data = data.copy()
    random.shuffle(shuffled_data)
    assert median(shuffled_data) == original_median, "Median should be invariant to shuffling"

@given(st.lists(st.integers(), max_size=0))
def test_empty_data_error(empty_list):
    """Test that median raises StatisticsError for empty data sequences.
    
    This property-based test verifies that the median function properly raises
    a StatisticsError when given any empty sequence, which is the expected
    behavior according to the statistics module specification.
    """
    import pytest
    import statistics
    
    with pytest.raises(statistics.StatisticsError):
        statistics.median(empty_list)

@given(st.lists(st.integers(), min_size=1).filter(lambda x: len(x) % 2 == 1))
def test_odd_length_returns_middle_element(data):
    """Test that for odd length data, median returns the middle element.
    
    For a sorted list with odd length, the median should be exactly the element
    at the middle position (index len(data) // 2).
    """
    sorted_data = sorted(data)
    expected_median = sorted_data[len(sorted_data) // 2]
    result = median(data)
    assert result == expected_median

@given(st.integers(min_value=1, max_value=50).flatmap(lambda n: st.lists(st.integers(), min_size=2*n, max_size=2*n)))
def test_even_length_interpolation(data):
    """Test that for even length data, median is average of two middle elements."""
    import math
    sorted_data = sorted(data)
    n = len(sorted_data)
    expected = (sorted_data[n//2 - 1] + sorted_data[n//2]) / 2
    result = median(data)
    assert math.isclose(result, expected, rel_tol=1e-9, abs_tol=1e-9)

@given(st.integers(min_value=-(2**53), max_value=2**53), st.integers(min_value=1, max_value=20))
def test_duplicate_handling(x, count):
    """Test that median of all identical elements returns that element.
    
    This property verifies that when all elements in a list are identical,
    the median function correctly returns that element. The integer range
    is restricted to values that can be exactly represented as floats
    (within 2^53) to avoid floating-point precision issues.
    """
    data = [x] * count
    assert median(data) == x

@given(st.integers(min_value=-10**10, max_value=10**10), st.integers(min_value=-10**10, max_value=10**10))
def test_two_element_average(a, b):
    """Test that median of two elements is their average.
    
    This property tests that for any two numbers, the median should equal
    their arithmetic mean. We use bounded integers to avoid overflow issues
    when computing (a + b) / 2 for very large integer values.
    """
    import math
    result = median([a, b])
    expected = (a + b) / 2
    assert math.isclose(result, expected)

@given(st.lists(st.integers(), min_size=1, max_size=10), 
       st.lists(st.integers(), min_size=1, max_size=10))
def test_monotonicity_preservation(data1, data2_raw):
    """Test that if all elements in data1 ≤ corresponding elements in data2, then median(data1) ≤ median(data2)."""
    # Ensure both lists have the same length by truncating to the shorter length
    min_len = min(len(data1), len(data2_raw))
    data1 = data1[:min_len]
    data2_raw = data2_raw[:min_len]
    
    # Create data2 where each element is >= corresponding element in data1
    # This ensures monotonicity: data2[i] >= data1[i] for all i
    data2 = [max(x, y) for x, y in zip(data1, data2_raw)]
    
    # Test the monotonicity property: if data1 ≤ data2 elementwise, then median(data1) ≤ median(data2)
    assert median(data1) <= median(data2)

@given(st.lists(st.integers(min_value=-100, max_value=100), min_size=1), 
       st.integers(min_value=1, max_value=10))
def test_median_positive_scaling_property(data, k):
    """Test that median([k*x for x in data]) == k * median(data) for k > 0.
    
    This property holds because scaling all elements by a positive constant
    preserves their relative order, so the median element(s) scale by the 
    same factor. This is a fundamental property of the median function
    under positive scalar multiplication.
    
    Args:
        data: A non-empty list of integers to test
        k: A positive integer scaling factor (k > 0)
    """
    import math
    
    # Scale all elements by k
    scaled_data = [k * x for x in data]
    
    # Calculate median of scaled data
    result = median(scaled_data)
    
    # Calculate k times the median of original data
    expected = k * median(data)
    
    # Use math.isclose for robust floating-point comparison
    assert math.isclose(result, expected), f"median({scaled_data}) = {result}, but k * median({data}) = {expected}"

import pytest
from statistics import StatisticsError, mode

@given(st.just([]))
def test_empty_data_raises_statistics_error(data):
    """Test that empty data raises StatisticsError when calculating mode.
    
    Property being tested: The mode function should raise a StatisticsError
    with the message "no mode for empty data" when given an empty list.
    """
    with pytest.raises(StatisticsError, match="no mode for empty data"):
        mode(data)

@given(st.lists(st.integers(), min_size=1))
def test_return_value_is_mode_of_input_data_integers(data):
    """Test that the returned mode is the most frequent element from the input data."""
    result = mode(data)
    
    # Verify the result is in the input data
    assert result in data
    
    # Verify the result is actually the mode (most frequent element)
    from collections import Counter
    counter = Counter(data)
    max_count = max(counter.values())
    most_frequent_elements = [elem for elem, count in counter.items() if count == max_count]
    
    # The result should be one of the most frequent elements
    assert result in most_frequent_elements

from collections import Counter

@given(st.lists(st.text(), min_size=1))
def test_return_value_is_element_from_input_data_strings(data):
    """Test that the returned mode is the most frequent element from the input data (strings)."""
    result = mode(data)
    
    # First verify the result exists in the input data
    assert result in data
    
    # Verify that the result is actually a mode (most frequent element)
    counter = Counter(data)
    max_count = max(counter.values())
    assert counter[result] == max_count, f"Result '{result}' has count {counter[result]}, but max count is {max_count}"

@given(st.lists(st.text(min_size=1), min_size=2))
def test_first_occurrence_wins_for_ties(data):
    """Test that when multiple elements have the same maximum frequency, the first one encountered wins."""
    # Create a scenario where we know there will be ties
    # Take the first two unique elements and make them appear with the same (maximum) frequency
    unique_elements = list(dict.fromkeys(data))  # preserve order, remove duplicates
    if len(unique_elements) < 2:
        return  # Skip if we don't have enough unique elements
    
    # Create a list where first two unique elements appear twice each, others appear once
    first_element = unique_elements[0]
    second_element = unique_elements[1]
    remaining_elements = unique_elements[2:]
    
    # Construct test data: first_element appears first, both tie for max frequency
    test_data = [first_element, second_element, first_element, second_element] + remaining_elements
    
    result = mode(test_data)
    # The first element should win the tie
    assert result == first_element

@given(st.lists(st.integers(), min_size=1))
def test_works_with_integers(data):
    """Test that mode works with integer data and returns actual mode(s).
    
    This test verifies that:
    1. The mode function returns integer(s) that exist in the input data
    2. The returned value(s) are actually the most frequent element(s)
    3. The function handles both single and multiple mode cases correctly
    """
    result = mode(data)
    
    # Handle both single mode and multiple modes cases
    if isinstance(result, list):
        modes = result
    else:
        modes = [result]
    
    # All returned modes should be integers from the data
    for m in modes:
        assert isinstance(m, int), f"Mode {m} is not an integer"
        assert m in data, f"Mode {m} is not present in the input data"
    
    # Verify these are actually modes (most frequent values)
    from collections import Counter
    counts = Counter(data)
    max_count = max(counts.values())
    expected_modes = [val for val, count in counts.items() if count == max_count]
    
    # The returned modes should exactly match the mathematically correct modes
    assert set(modes) == set(expected_modes), \
        f"Expected modes {expected_modes}, but got {modes}"
    
    # Ensure no duplicates in the result if it's a list
    if isinstance(result, list):
        assert len(modes) == len(set(modes)), "Duplicate modes returned"

@given(st.lists(st.text(), min_size=1))
def test_works_with_strings(data):
    """Test that mode works with string data and returns the most frequent element.
    
    This test verifies that:
    1. The mode function returns a string when given string data
    2. The returned value is present in the original data
    3. The returned value has the maximum frequency count (is actually a mode)
    """
    result = mode(data)
    assert isinstance(result, str)
    assert result in data
    
    # Count frequencies to verify the result is actually a mode
    from collections import Counter
    counts = Counter(data)
    max_count = max(counts.values())
    
    # The result should be one of the elements with maximum frequency
    assert counts[result] == max_count

@given(st.lists(st.tuples(st.integers(), st.text()), min_size=1))
def test_works_with_tuples(data):
    """Test that mode works with tuple data (hashable) and returns the most frequent element."""
    result = mode(data)
    
    # Verify the result is a tuple and exists in the input data
    assert isinstance(result, tuple)
    assert result in data
    
    # Count frequencies to verify it's actually the mode
    from collections import Counter
    counts = Counter(data)
    max_count = max(counts.values())
    
    # The returned tuple must have the maximum frequency count
    assert counts[result] == max_count, f"Result {result} appears {counts[result]} times, but max frequency is {max_count}"
    
    # Additional verification: ensure no other element has a higher frequency
    for element, count in counts.items():
        assert count <= counts[result], f"Element {element} has count {count} which is greater than result {result} with count {counts[result]}"

@given(st.lists(st.integers(), min_size=1))
def test_returned_element_has_maximum_frequency(data):
    """Test that the returned element has the maximum frequency in the data.
    
    This test verifies that:
    1. The returned element actually appears in the input data
    2. The returned element has the maximum frequency among all elements
    3. The returned element is one of the valid mode candidates (handles ties correctly)
    """
    from collections import Counter
    
    result = mode(data)
    counter = Counter(data)
    max_frequency = max(counter.values())
    
    # Get all elements that have the maximum frequency (valid mode candidates)
    elements_with_max_freq = [elem for elem, freq in counter.items() if freq == max_frequency]
    
    # The result should be one of the elements with maximum frequency
    assert result in elements_with_max_freq, f"Result {result} is not among valid mode candidates {elements_with_max_freq}"
    
    # Also verify it actually has the maximum frequency
    assert counter[result] == max_frequency, f"Result {result} has frequency {counter[result]}, but max frequency is {max_frequency}"

@given(st.lists(st.integers(), min_size=1))
def test_mode_is_most_frequent_element(data):
    """Test that the mode function returns the most frequently occurring element(s)."""
    result = mode(data)
    
    # Count frequencies to determine expected modes
    from collections import Counter
    counts = Counter(data)
    max_count = max(counts.values())
    expected_modes = [item for item, count in counts.items() if count == max_count]
    
    # The result should be one of the most frequent elements
    # (in case of ties, different implementations might return different valid modes)
    if isinstance(result, list):
        # If function returns a list of modes
        assert len(result) > 0, "Mode function should return at least one mode"
        assert all(r in expected_modes for r in result), "All returned modes should be among the most frequent elements"
    else:
        # If function returns a single mode
        assert result in expected_modes, "Returned mode should be one of the most frequent elements"

@given(st.lists(st.text(min_size=1, max_size=10), min_size=2, max_size=20))
def test_order_preservation_for_tie_breaking(data):
    """Test that mode preserves order for tie-breaking - first occurrence wins among tied elements.
    
    This property-based test verifies that when multiple elements have the same highest frequency,
    the mode function returns the element that appears first in the original list.
    """
    result = mode(data)
    
    # Count frequencies and track first occurrence position for each element
    counts = {}
    first_occurrence = {}
    for i, item in enumerate(data):
        if item not in counts:
            counts[item] = 0
            first_occurrence[item] = i
        counts[item] += 1
    
    # Find the maximum frequency
    max_freq = max(counts.values())
    
    # Find all elements that have the maximum frequency (tied for mode)
    tied_items = [item for item, count in counts.items() if count == max_freq]
    
    # The result should be one of the tied items
    assert result in tied_items, f"Result {result} is not among the most frequent items {tied_items}"
    
    # If there are multiple tied items, the result should be the one that appears first
    if len(tied_items) > 1:
        expected_winner = min(tied_items, key=lambda x: first_occurrence[x])
        assert result == expected_winner, (
            f"Expected {expected_winner} (first occurrence at index {first_occurrence[expected_winner]}) "
            f"to win tie-breaking, but got {result} (first occurrence at index {first_occurrence[result]})"
        )
    
    # Verify that the result actually appears in the original data
    assert result in data, f"Result {result} does not appear in the input data {data}"
    
    # Verify that the result has the maximum frequency
    assert counts[result] == max_freq, (
        f"Result {result} has frequency {counts[result]}, but maximum frequency is {max_freq}"
    )

@given(st.lists(st.integers(), min_size=1, max_size=10))
def test_mode_with_tie_scenario(data):
    """Test mode when all elements appear exactly once (tie scenario)."""
    # Create data where each element appears exactly once
    unique_data = list(set(data))
    
    if len(unique_data) >= 2:
        # Tie scenario: all elements appear exactly once
        result = mode(unique_data)
        # In a tie scenario, result should be one of the elements in the list
        assert result in unique_data, f"Mode {result} should be one of the elements {unique_data}"
    elif len(unique_data) == 1:
        # Single element case: no tie, clear mode
        result = mode(unique_data)
        assert result == unique_data[0], f"Mode should be the only element {unique_data[0]}"

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e6, max_value=1e6), min_size=3, max_size=20))
def test_stdev_properties(data):
    """Test mathematical properties of standard deviation."""
    import math
    
    result = stdev(data)
    
    # Standard deviation should always be non-negative
    assert result >= 0
    
    # If all values are the same, standard deviation should be 0
    if len(set(data)) == 1:
        assert result == 0
    else:
        # If values differ, standard deviation should be positive
        assert result > 0
    
    # Standard deviation should be scale-invariant for multiplication
    # stdev([k*x for x in data]) should equal |k| * stdev(data)
    if result > 0:  # Only test scaling when there's variation in the data
        scaled_data = [2 * x for x in data]
        scaled_result = stdev(scaled_data)
        assert math.isclose(scaled_result, 2 * result, rel_tol=1e-9)
    
    # Standard deviation should be translation-invariant
    # stdev([x + c for x in data]) should equal stdev(data)
    translated_data = [x + 100 for x in data]
    translated_result = stdev(translated_data)
    assert math.isclose(translated_result, result, rel_tol=1e-9)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0, max_size=1))
def test_stdev_error_condition_insufficient_data(data):
    """Test that stdev raises StatisticsError when given fewer than 2 data points.
    
    Standard deviation requires at least 2 data points to calculate the variance
    from the mean. Both empty lists and single-element lists should raise
    StatisticsError since there's insufficient data to compute a meaningful
    standard deviation.
    """
    import pytest
    from statistics import StatisticsError
    
    with pytest.raises(StatisticsError):
        stdev(data)

@given(
    st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e6, max_value=1e6), min_size=2, max_size=10),
    st.floats(allow_nan=False, allow_infinity=False, min_value=-100, max_value=100).filter(lambda x: x != 0)
)
def test_stdev_scale_invariance(data, k):
    """Test that stdev([k*x for x in data]) == abs(k) * stdev(data) for k != 0.
    
    This tests the mathematical property that standard deviation scales linearly
    with the absolute value of the scaling factor. This is a fundamental property
    of standard deviation that should hold for any non-zero scaling factor k.
    """
    import math
    original_stdev = stdev(data)
    scaled_data = [k * x for x in data]
    scaled_stdev = stdev(scaled_data)
    expected = abs(k) * original_stdev
    # Use a more reasonable tolerance to account for floating-point precision issues
    # that can accumulate through multiplication and standard deviation calculations
    assert math.isclose(scaled_stdev, expected, rel_tol=1e-9)

@given(
    st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e3, max_value=1e3), min_size=2, max_size=10),
    st.floats(allow_nan=False, allow_infinity=False, min_value=-100, max_value=100)
)
def test_stdev_translation_invariance(data, c):
    """Test that stdev([x + c for x in data]) == stdev(data).
    
    Translation invariance is a fundamental property of standard deviation:
    adding a constant to all values should not change the spread (standard deviation).
    This test verifies this mathematical property while handling floating-point
    precision limitations by filtering out cases where extremely small values
    would cause numerical instability when combined with larger translation constants.
    """
    import math
    
    # Skip cases where data contains values that are extremely small relative to c
    # This prevents floating-point precision issues where tiny values become
    # negligible compared to the translation constant
    if c != 0 and any(abs(x) < abs(c) * 1e-12 and x != 0 for x in data):
        return
    
    # Skip cases where all values are effectively zero (would result in zero stdev)
    if all(abs(x) < 1e-15 for x in data):
        return
    
    original_stdev = stdev(data)
    translated_data = [x + c for x in data]
    translated_stdev = stdev(translated_data)
    
    # Use both relative and absolute tolerance to handle cases where stdev is very small
    assert math.isclose(translated_stdev, original_stdev, rel_tol=1e-9, abs_tol=1e-12), \
        f"Translation invariance failed: original_stdev={original_stdev}, translated_stdev={translated_stdev}, data={data}, c={c}"

@given(
    st.floats(allow_nan=False, allow_infinity=False, min_value=-1000, max_value=1000),
    st.integers(min_value=2, max_value=10)
)
def test_stdev_zero_deviation_constant_data(c, n):
    """Test that stdev of constant data equals zero.
    
    Property: When all data points are identical, the standard deviation
    should be zero since there is no variation in the data.
    Uses n >= 2 to ensure valid standard deviation calculation.
    """
    import math
    data = [c] * n
    result = stdev(data)
    # Use a more reasonable tolerance for floating-point comparisons
    # to account for potential precision errors in calculations
    assert math.isclose(result, 0, abs_tol=1e-12)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1000, max_value=1000), min_size=2, max_size=10))
def test_stdev_consistency_with_xbar_parameter(data):
    """Test that stdev(data, xbar) == stdev(data) when xbar equals the actual mean.
    
    This property verifies that providing the correct mean as the xbar parameter
    should produce the same result as letting stdev calculate the mean internally.
    The test accounts for floating-point precision issues that can occur when
    dealing with numbers of vastly different magnitudes by using appropriate
    tolerances in the comparison.
    """
    import math
    mean_value = sum(data) / len(data)
    stdev_without_xbar = stdev(data)
    stdev_with_xbar = stdev(data, xbar=mean_value)
    assert math.isclose(stdev_without_xbar, stdev_with_xbar, rel_tol=1e-9, abs_tol=1e-15)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-100, max_value=100), min_size=2, max_size=10))
def test_stdev_relationship_to_variance(data):
    """
    Test that stdev(data) == sqrt(variance(data)).
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance. Uses relaxed tolerance to account for
    floating-point precision errors that can accumulate through square root
    operations.
    """
    import math
    stdev_result = stdev(data)
    variance_result = variance(data)
    expected = math.sqrt(variance_result)
    
    # Use more reasonable tolerances to handle floating-point precision errors
    # rel_tol=1e-12 for relative tolerance and abs_tol=1e-15 for cases near zero
    assert math.isclose(stdev_result, expected, rel_tol=1e-12, abs_tol=1e-15), \
        f"stdev({data}) = {stdev_result}, but sqrt(variance({data})) = {expected}"

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=50))
def test_variance_non_negative(data):
    """Test that variance is always non-negative.
    
    The variance of any dataset should always be >= 0, as it represents
    the average of squared deviations from the mean. We constrain the
    input values to a reasonable range (-1e10 to 1e10) to avoid
    numerical overflow during variance calculations while still
    testing the fundamental mathematical property.
    """
    result = variance(data)
    assert result >= 0, f"Variance should be non-negative, but got {result} for data: {data}"

@given(st.just([]))
def test_variance_error_empty_data(data):
    """Test that variance raises StatisticsError when given empty data."""
    import pytest
    from statistics import StatisticsError
    with pytest.raises(StatisticsError):
        variance(data)


@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1))
def test_variance_error_single_element(data):
    """Test that variance raises StatisticsError when given single element data."""
    import pytest
    from statistics import StatisticsError
    with pytest.raises(StatisticsError):
        variance(data)


@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=10))
def test_variance_sufficient_data(data):
    """Test that variance works correctly when given sufficient data (2 or more elements)."""
    # This should not raise an exception and should return a non-negative float
    result = variance(data)
    assert isinstance(result, (int, float))
    assert result >= 0  # Variance is always non-negative

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=50))
def test_variance_mean_consistency(data):
    """Test that variance with explicit mean equals variance with computed mean.
    
    This property verifies that calculating variance by letting the function
    compute the mean internally produces the same result as explicitly passing
    the mean as a parameter. Uses relaxed tolerances to account for floating-point
    precision issues that can occur with very large numbers during intermediate
    variance calculations.
    """
    import math
    from statistics import mean
    computed_variance = variance(data)
    explicit_mean_variance = variance(data, mean(data))
    assert math.isclose(computed_variance, explicit_mean_variance, rel_tol=1e-9, abs_tol=1e-10)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e6, max_value=1e6), min_size=2, max_size=20),
       st.floats(allow_nan=False, allow_infinity=False, min_value=-10, max_value=10).filter(lambda x: abs(x) > 1e-6))
def test_variance_scale_invariance(data, scale_factor):
    """Test that variance scales by the square of the scaling factor.
    
    This property test verifies that when all data points are multiplied by a constant
    scale factor, the variance of the scaled data equals the original variance multiplied
    by the square of the scale factor: Var(c*X) = c²*Var(X).
    
    Uses a more lenient relative tolerance (1e-6) to account for accumulated floating-point
    errors in variance calculations which involve multiple arithmetic operations.
    """
    import math
    original_variance = variance(data)
    scaled_data = [scale_factor * x for x in data]
    scaled_variance = variance(scaled_data)
    expected_variance = (scale_factor ** 2) * original_variance
    assert math.isclose(scaled_variance, expected_variance, rel_tol=1e-6)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=50),
       st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10))
def test_variance_translation_invariance(data, translation):
    """Test that variance is unchanged by adding a constant to all data points.
    
    This tests the mathematical property that Var(X + c) = Var(X) for any constant c.
    The variance should be translation invariant because adding a constant shifts
    all values equally, which doesn't change the spread of the data.
    
    We limit the range of floats to avoid numerical precision issues that can occur
    with extremely large floating-point numbers during variance calculations.
    """
    import math
    original_variance = variance(data)
    translated_data = [x + translation for x in data]
    translated_variance = variance(translated_data)
    
    # Use a more lenient relative tolerance since we're dealing with floating-point arithmetic
    # and variance calculations involve squared differences which can amplify small errors
    assert math.isclose(original_variance, translated_variance, rel_tol=1e-9, abs_tol=1e-15)

@given(st.lists(st.integers(min_value=-100, max_value=100), min_size=2, max_size=20))
def test_variance_mathematical_property_integers(data):
    """Test that variance calculation is mathematically correct for integers."""
    result = variance(data)
    
    # Variance should be non-negative
    assert result >= 0
    
    # Calculate expected variance manually using the standard formula
    mean = sum(data) / len(data)
    expected_variance = sum((x - mean) ** 2 for x in data) / len(data)
    
    # Compare with tolerance for floating point arithmetic
    assert abs(result - expected_variance) < 1e-10
    
    # Result should be numeric (variance of integers can be float due to division)
    assert isinstance(result, (int, float))
    
    # Variance should be zero if and only if all elements are equal
    all_equal = len(set(data)) == 1
    if all_equal:
        assert result == 0
    else:
        assert result > 0

@given(st.lists(st.fractions(min_value=-10, max_value=10), min_size=2, max_size=10))
def test_variance_type_preservation_fractions(data):
    """Test that variance works with Fraction types and returns correct values."""
    from fractions import Fraction
    result = variance(data)
    
    # Check type preservation
    assert isinstance(result, Fraction)
    
    # Check correctness by calculating expected variance
    mean = sum(data) / len(data)
    expected_variance = sum((x - mean) ** 2 for x in data) / len(data)
    assert result == expected_variance

@given(st.fractions(min_value=-10, max_value=10), st.integers(min_value=2, max_value=10))
def test_variance_identical_fractions(value, size):
    """Test variance of identical Fraction values is zero."""
    from fractions import Fraction
    data = [value] * size
    result = variance(data)
    assert isinstance(result, Fraction)
    assert result == Fraction(0)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=1e100), min_size=2, max_size=20))
def test_variance_order_independence(data):
    """Test that variance is independent of data order.
    
    This property test verifies that the variance calculation produces the same result
    regardless of the order of elements in the input data. We restrict the float range
    to avoid extreme values that could cause numerical precision issues, and use a
    reasonable tolerance to account for floating-point arithmetic limitations.
    """
    import math
    import random
    
    original_variance = variance(data)
    shuffled_data = data.copy()
    random.shuffle(shuffled_data)
    shuffled_variance = variance(shuffled_data)
    
    # Use a more reasonable tolerance to account for floating-point precision
    # when dealing with variance calculations involving large numbers
    assert math.isclose(original_variance, shuffled_variance, rel_tol=1e-9)

import math

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=1e100), min_size=2, max_size=25))
def test_variance_duplicate_value_handling(data):
    """Test that variance handles duplicated datasets appropriately.
    
    The key property being tested is that duplicating all values in a dataset
    should not change the variance, since variance measures the spread of data
    around the mean, not the sample size. When we duplicate data, we get the
    same mean and the same spread, so variance should remain unchanged.
    """
    original_variance = variance(data)
    duplicated_data = data + data
    duplicated_variance = variance(duplicated_data)
    
    # Both should be non-negative (variance is always >= 0)
    assert original_variance >= 0
    assert duplicated_variance >= 0
    
    # Key property: duplicating data should not change the variance
    # because variance measures spread, not sample size
    assert math.isclose(original_variance, duplicated_variance, rel_tol=1e-10)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=20))
def test_variance_minimum_bound_equality_condition(data):
    """Test that variance equals zero if and only if all elements are equal."""
    import math
    result = variance(data)
    all_equal = len(set(data)) <= 1
    variance_is_zero = math.isclose(result, 0, abs_tol=1e-10)
    assert all_equal == variance_is_zero

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=50))
def test_stdev_equals_sqrt_of_variance_floats(data):
    """Test that stdev(data) == sqrt(variance(data)) for floating point data.
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance: stdev is the square root of variance.
    
    The strategy is constrained to reasonable floating-point ranges (-1e10 to 1e10)
    to avoid precision issues with extremely large numbers that can cause
    floating-point arithmetic limitations in the square root calculation.
    """
    import math
    
    stdev_result = stdev(data)
    variance_result = variance(data)
    sqrt_variance = math.sqrt(variance_result)
    
    # Use a more reasonable tolerance that accounts for floating-point precision
    # while still being strict enough to catch real implementation errors
    assert math.isclose(stdev_result, sqrt_variance, rel_tol=1e-9)

@given(st.lists(st.decimals(min_value=-1000, max_value=1000, allow_nan=False, allow_infinity=False), min_size=2, max_size=20))
def test_stdev_equals_sqrt_of_variance_decimals(data):
    """Test that stdev(data) == sqrt(variance(data)) for Decimal data.
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance with Decimal inputs, which should
    maintain high precision. Handles edge cases where variance is zero
    (all values identical) explicitly.
    """
    from decimal import Decimal
    
    stdev_result = stdev(data)
    variance_result = variance(data)
    
    # Handle the edge case where all values are identical (variance = 0)
    if variance_result == Decimal('0'):
        # When variance is zero, standard deviation should also be zero
        assert stdev_result == Decimal('0'), f"Expected stdev=0 when variance=0, got stdev={stdev_result}"
    else:
        # For non-zero variance, test the fundamental relationship
        sqrt_variance = variance_result.sqrt()
        assert stdev_result == sqrt_variance, f"stdev={stdev_result} != sqrt(variance)={sqrt_variance}"

@given(
    st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=50),
    st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10)
)
def test_stdev_equals_sqrt_of_variance_with_xbar(data, xbar):
    """Test that stdev(data, xbar) == sqrt(variance(data, xbar)) when xbar is provided.
    
    This property tests the mathematical relationship holds even when
    an explicit mean (xbar) is provided to both functions.
    
    The floating-point values are restricted to a reasonable range to avoid
    numerical instability that occurs with extremely large values where
    variance calculations can exceed floating-point precision limits.
    """
    import math
    
    stdev_result = stdev(data, xbar)
    variance_result = variance(data, xbar)
    sqrt_variance = math.sqrt(variance_result)
    
    # Use adaptive tolerance based on the magnitude of the result
    # to account for floating-point precision limitations
    tolerance = max(1e-12, abs(stdev_result) * 1e-9)
    
    assert math.isclose(stdev_result, sqrt_variance, rel_tol=1e-9, abs_tol=tolerance)

@given(st.lists(st.fractions(min_value=-100, max_value=100), min_size=2, max_size=20))
def test_stdev_equals_sqrt_of_variance_fractions(data):
    """Test that stdev(data) == sqrt(variance(data)) for Fraction data.
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance with Fraction inputs. Special handling
    is needed for the case where variance is 0 (all values identical) and
    for maintaining precision when working with Fraction arithmetic.
    """
    from fractions import Fraction
    import math
    
    stdev_result = stdev(data)
    variance_result = variance(data)
    
    # Handle the case where variance is 0 (all values are the same)
    if variance_result == 0:
        # When variance is 0, standard deviation must also be 0
        assert stdev_result == 0
    else:
        # For non-zero variance, we need to compare stdev with sqrt(variance)
        # Since we're dealing with Fractions, we want to maintain precision
        # but sqrt operations require floating point arithmetic
        
        # Convert to float for sqrt calculation, then back to Fraction
        sqrt_variance_float = math.sqrt(float(variance_result))
        
        # Convert both results to float for comparison to avoid precision issues
        # when converting between Fraction and float representations
        stdev_float = float(stdev_result)
        
        # Use a reasonable tolerance that accounts for floating point conversion
        # but is still strict enough to catch real errors
        assert math.isclose(stdev_float, sqrt_variance_float, rel_tol=1e-9), \
            f"stdev({stdev_float}) != sqrt(variance)({sqrt_variance_float})"

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=100))
def test_variance_with_explicit_mean_equals_variance_with_none(data):
    """
    Test that variance(data, mean(data)) == variance(data, None).
    
    This property verifies that explicitly passing the mean as the xbar parameter
    produces identical results to letting variance calculate the mean internally.
    The variance function should behave identically whether the mean is provided
    or computed automatically.
    
    Note: We restrict the floating-point range to [-1e10, 1e10] to avoid
    numerical instability issues that can occur with extremely large numbers,
    which would cause legitimate differences in floating-point arithmetic
    depending on the computational path taken.
    """
    import math
    
    # Calculate the mean of the data
    computed_mean = mean(data)
    
    # Get variance with explicit mean
    variance_with_mean = variance(data, computed_mean)
    
    # Get variance with None (auto-calculated mean)
    variance_with_none = variance(data, None)
    
    # They should be equal (using close comparison for floating point)
    # Using a slightly more lenient tolerance to account for expected
    # floating-point precision differences in the computation paths
    assert math.isclose(variance_with_mean, variance_with_none, rel_tol=1e-12)

import math
from hypothesis import given, strategies as st

@given(st.lists(st.integers(min_value=-1000, max_value=1000), min_size=2, max_size=50))
def test_variance_with_explicit_mean_equals_variance_with_none_integers(data):
    """
    Test that variance calculations with explicit mean and auto-calculated mean 
    produce equivalent results.
    
    Even with integer inputs, variance calculations involve division operations
    that can produce floating point results, so we use math.isclose() for
    appropriate floating point comparison rather than exact equality.
    """
    # Calculate the mean of the data
    computed_mean = mean(data)
    
    # Get variance with explicit mean
    variance_with_mean = variance(data, computed_mean)
    
    # Get variance with None (auto-calculated mean)
    variance_with_none = variance(data, None)
    
    # Use floating point comparison since variance calculations can produce
    # floating point results even with integer inputs
    assert math.isclose(variance_with_mean, variance_with_none, rel_tol=1e-15)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=1e100), min_size=2, max_size=100))
def test_stdev_with_explicit_mean_equals_stdev_with_none(data):
    """
    Test that stdev(data, mean(data)) == stdev(data, None).
    
    This property tests that explicitly passing the mean as the xbar parameter
    produces the same result as letting stdev calculate it internally (None).
    
    The input is constrained to avoid extremely large values that cause numerical
    instability in floating point arithmetic, and uses a reasonable tolerance
    for comparison to account for precision differences between the two approaches.
    """
    import math
    from statistics import stdev, mean
    
    # Calculate stdev with explicit mean
    data_mean = mean(data)
    stdev_with_mean = stdev(data, data_mean)
    
    # Calculate stdev with None (let it calculate mean internally)
    stdev_with_none = stdev(data, None)
    
    # Compare using math.isclose with reasonable tolerance for floating point comparison
    # Use rel_tol=1e-9 to account for numerical instability with large values
    assert math.isclose(stdev_with_mean, stdev_with_none, rel_tol=1e-9), \
        f"stdev with explicit mean {stdev_with_mean} != stdev with None {stdev_with_none}"

import math

@given(st.lists(st.integers(-1000, 1000), min_size=2, max_size=50))
def test_stdev_with_explicit_mean_equals_stdev_with_none_integers(data):
    """
    Test that stdev(data, mean(data)) ≈ stdev(data, None) for integer data.
    
    This tests that calculating standard deviation with an explicitly provided mean
    produces approximately the same result as letting stdev calculate the mean internally.
    The results should be very close but may differ slightly due to floating-point
    precision differences in how the mean is calculated and used.
    """
    from statistics import stdev, mean
    
    # Calculate stdev with explicit mean
    data_mean = mean(data)
    stdev_with_mean = stdev(data, data_mean)
    
    # Calculate stdev with None (let it calculate mean internally)
    stdev_with_none = stdev(data, None)
    
    # Use approximate equality to account for floating-point precision issues
    assert math.isclose(stdev_with_mean, stdev_with_none, rel_tol=1e-15), \
        f"stdev with explicit mean {stdev_with_mean} != stdev with None {stdev_with_none}"

@given(st.one_of(
    st.integers(),
    st.floats(allow_nan=False, allow_infinity=False),
    st.fractions(),
    st.decimals(allow_nan=False, allow_infinity=False),
    st.text(min_size=1),  # Exclude empty strings to avoid StatisticsError in mode()
    st.booleans()
))
def test_single_element_central_tendency_equality(x):
    """
    Test that for single-element data [x], mean([x]) == median([x]) == mode([x]) == x.
    
    This property should hold for any single value regardless of type, as all three
    measures of central tendency collapse to the single value when there's only one
    data point.
    
    Note: Empty strings are excluded from this test because statistics.mode() 
    treats them as a special case and raises StatisticsError.
    """
    from statistics import mean, median, mode
    import math
    from decimal import Decimal
    from fractions import Fraction
    
    single_element_data = [x]
    
    mean_result = mean(single_element_data)
    median_result = median(single_element_data)
    mode_result = mode(single_element_data)
    
    # For numeric types, we need to handle floating point comparison carefully
    if isinstance(x, float):
        # All results should be close to x and to each other
        assert math.isclose(mean_result, x, rel_tol=1e-15)
        assert math.isclose(median_result, x, rel_tol=1e-15)
        assert mode_result == x  # mode should return exact value
        assert math.isclose(mean_result, median_result, rel_tol=1e-15)
        assert math.isclose(mean_result, mode_result, rel_tol=1e-15)
    else:
        # For non-float types, exact equality should hold
        assert mean_result == x
        assert median_result == x
        assert mode_result == x
        assert mean_result == median_result == mode_result

@given(st.one_of(
    st.integers(),
    st.floats(allow_nan=False, allow_infinity=False)
), st.integers(min_value=1, max_value=100))
def test_constant_numeric_data_central_tendency_equality(constant_value, list_length):
    """
    Test that for constant numeric data [x, x, ..., x], all measures of central tendency
    (mean, median, mode) equal the constant value x.
    
    This property should hold for any constant numeric value (integers and floats)
    and any positive list length.
    """
    from statistics import mean, median, mode
    import math
    
    # Create constant data list
    constant_data = [constant_value] * list_length
    
    # Calculate all measures of central tendency
    calculated_mean = mean(constant_data)
    calculated_median = median(constant_data)
    calculated_mode = mode(constant_data)
    
    # For floating point numbers, use approximate equality due to potential precision issues
    if isinstance(constant_value, float):
        assert math.isclose(calculated_mean, constant_value), f"Mean {calculated_mean} != {constant_value}"
        assert math.isclose(calculated_median, constant_value), f"Median {calculated_median} != {constant_value}"
        assert math.isclose(calculated_mode, constant_value), f"Mode {calculated_mode} != {constant_value}"
        
        # Verify that all three measures are equal to each other
        assert math.isclose(calculated_mean, calculated_median), f"Mean {calculated_mean} != Median {calculated_median}"
        assert math.isclose(calculated_median, calculated_mode), f"Median {calculated_median} != Mode {calculated_mode}"
        assert math.isclose(calculated_mean, calculated_mode), f"Mean {calculated_mean} != Mode {calculated_mode}"
    else:
        # For exact types (int), use exact equality
        assert calculated_mean == constant_value, f"Mean {calculated_mean} != {constant_value}"
        assert calculated_median == constant_value, f"Median {calculated_median} != {constant_value}"
        assert calculated_mode == constant_value, f"Mode {calculated_mode} != {constant_value}"
        
        # Verify that all three measures are equal to each other
        assert calculated_mean == calculated_median == calculated_mode, f"Not all equal: mean={calculated_mean}, median={calculated_median}, mode={calculated_mode}"


@given(st.one_of(
    st.text(),
    st.booleans()
), st.integers(min_value=1, max_value=100))
def test_constant_non_numeric_data_central_tendency_equality(constant_value, list_length):
    """
    Test that for constant non-numeric data [x, x, ..., x], median and mode
    equal the constant value x.
    
    This property should hold for any constant non-numeric value (strings and booleans)
    and any positive list length. Mean is not applicable for non-numeric data.
    """
    from statistics import median, mode
    
    # Create constant data list
    constant_data = [constant_value] * list_length
    
    # Calculate applicable measures of central tendency
    calculated_median = median(constant_data)
    calculated_mode = mode(constant_data)
    
    # For non-numeric types, use exact equality
    assert calculated_median == constant_value, f"Median {calculated_median} != {constant_value}"
    assert calculated_mode == constant_value, f"Mode {calculated_mode} != {constant_value}"
    
    # Verify that both measures are equal to each other
    assert calculated_median == calculated_mode, f"Median {calculated_median} != Mode {calculated_mode}"

@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), 
       st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10))
def test_variance_equals_stdev_squared_for_two_elements(a, b):
    """
    Test that for two-element data [a, b]: variance([a, b]) == stdev([a, b])^2
    
    This verifies the fundamental relationship between variance and standard deviation
    (stdev = sqrt(variance)) for the minimal case where both functions are defined
    (requiring at least 2 data points).
    
    Note: Float range is limited to avoid extreme values that cause numerical
    precision issues in floating point arithmetic.
    """
    import math
    from statistics import variance, stdev
    
    data = [a, b]
    
    # Calculate variance and standard deviation
    var_result = variance(data)
    stdev_result = stdev(data)
    
    # Test the fundamental relationship: variance = stdev^2
    stdev_squared = stdev_result ** 2
    
    # Use math.isclose with appropriate tolerance for floating point comparison
    # rel_tol=1e-9 handles relative precision errors for larger values
    # abs_tol=1e-12 handles absolute precision errors for values close to zero
    assert math.isclose(var_result, stdev_squared, rel_tol=1e-9, abs_tol=1e-12), \
        f"variance([{a}, {b}]) = {var_result} != {stdev_squared} = stdev([{a}, {b}])^2"