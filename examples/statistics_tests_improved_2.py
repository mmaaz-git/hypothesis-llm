"""Property-based tests for statistics module.
Generated by hypothesis-llm.
"""

import hypothesis
from hypothesis import given, strategies as st
from statistics import (
    mean,
    median,
    mode,
    stdev,
    variance
)






@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))
def test_mean_bounded_by_min_max(data):
    """Test that min(data) ≤ mean(data) ≤ max(data)"""
    result = mean(data)
    assert min(data) <= result <= max(data)




@given(st.floats(allow_nan=False, allow_infinity=False))
def test_mean_single_element_invariant(x):
    """Test that mean([x]) == x for any single element"""
    import math
    result = mean([x])
    assert math.isclose(result, x, rel_tol=1e-9)




@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))
def test_mean_order_independence(data):
    """Test that mean(data) == mean(shuffled(data))"""
    import math
    import random
    
    original_mean = mean(data)
    shuffled_data = data.copy()
    random.shuffle(shuffled_data)
    shuffled_mean = mean(shuffled_data)
    
    assert math.isclose(original_mean, shuffled_mean, rel_tol=1e-9)



@given(st.lists(st.integers(), min_size=1))
def test_bounded_output(data):
    """Test that median is bounded by min and max of data."""
    result = median(data)
    assert min(data) <= result <= max(data)



@given(st.integers())
def test_single_element_identity(x):
    """Test that median of single element returns that element."""
    assert median([x]) == x




@given(st.integers())
def test_single_element_data_returns_that_element(element):
    """Test that single element data returns that element."""
    data = [element]
    result = mode(data)
    assert result == element



@given(st.floats(allow_nan=False, allow_infinity=False), st.integers(min_value=2, max_value=20))
def test_variance_zero_for_constant_data(constant_value, size):
    """Test that variance is zero when all data points are identical."""
    import math
    data = [constant_value] * size
    result = variance(data)
    assert math.isclose(result, 0, abs_tol=1e-10)




@given(st.lists(st.integers(min_value=-1000, max_value=1000), min_size=2, max_size=50))
def test_stdev_equals_sqrt_of_variance_integers(data):
    """Test that stdev(data) == sqrt(variance(data)) for integer data.
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance with integer inputs.
    """
    import math
    
    stdev_result = stdev(data)
    variance_result = variance(data)
    sqrt_variance = math.sqrt(variance_result)
    
    assert math.isclose(stdev_result, sqrt_variance, rel_tol=1e-12)




@given(st.lists(st.fractions(min_value=-100, max_value=100), min_size=2, max_size=20))
def test_variance_with_explicit_mean_equals_variance_with_none_fractions(data):
    """
    Test the same property with Fraction data for exact arithmetic.
    
    Using Fraction objects ensures exact arithmetic without floating point
    precision issues, allowing us to test for exact equality.
    """
    from fractions import Fraction
    
    # Calculate the mean of the data
    computed_mean = mean(data)
    
    # Get variance with explicit mean
    variance_with_mean = variance(data, computed_mean)
    
    # Get variance with None (auto-calculated mean)
    variance_with_none = variance(data, None)
    
    # They should be exactly equal for Fraction inputs
    assert variance_with_mean == variance_with_none




@given(st.lists(st.fractions(), min_size=2, max_size=20))
def test_stdev_with_explicit_mean_equals_stdev_with_none_fractions(data):
    """
    Test that stdev(data, mean(data)) == stdev(data, None) for Fraction data.
    
    This tests the same property but with Fraction input data to ensure
    the equivalence holds for exact rational arithmetic.
    """
    from statistics import stdev, mean
    
    # Calculate stdev with explicit mean
    data_mean = mean(data)
    stdev_with_mean = stdev(data, data_mean)
    
    # Calculate stdev with None (let it calculate mean internally)
    stdev_with_none = stdev(data, None)
    
    # For Fraction data, the results should be exactly equal
    assert stdev_with_mean == stdev_with_none, \
        f"stdev with explicit mean {stdev_with_mean} != stdev with None {stdev_with_none}"


@given(st.just([]))
def test_mean_empty_list_raises_error(data):
    """Test that mean([]) raises StatisticsError"""
    import pytest
    from statistics import mean, StatisticsError
    with pytest.raises(StatisticsError):
        mean(data)


@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))
def test_mean_float_input(data):
    """Test that mean of floats returns float and is mathematically correct"""
    result = mean(data)
    assert isinstance(result, float)
    # Verify correctness with tolerance for floating point arithmetic
    expected = sum(data) / len(data)
    assert math.isclose(result, expected, rel_tol=1e-9)


@given(st.floats(allow_nan=False, allow_infinity=False),
       st.integers(min_value=1, max_value=100))
def test_mean_duplicate_invariant(x, n):
    """Test that mean([x]*n) == x
    
    This property tests the invariant that the mean of n identical values
    should equal the original value. We use both relative and absolute
    tolerances to handle floating-point precision issues:
    - Relative tolerance handles larger values appropriately
    - Absolute tolerance handles cases where x is very small or zero
    """
    import math
    data = [x] * n
    result = mean(data)
    assert math.isclose(result, x, rel_tol=1e-9, abs_tol=1e-15)


@given(st.lists(st.decimals(allow_nan=False, allow_infinity=False, min_value=-1000, max_value=1000, places=2), min_size=1, max_size=100))
def test_mean_decimal_type_preservation(data):
    """Test that mean preserves Decimal type and computes correct value
    
    This test verifies two key properties:
    1. Type preservation: The result should be a Decimal when input is Decimal
    2. Correctness: The computed mean should equal the mathematical mean
    
    The strategy uses bounded decimals to avoid overflow issues and extreme
    precision problems that could affect the calculation.
    """
    from decimal import Decimal
    
    result = mean(data)
    
    # Test type preservation - result should be Decimal type
    assert isinstance(result, Decimal), f"Expected Decimal, got {type(result)}"
    
    # Test correctness of the mean calculation
    # Use Decimal arithmetic throughout to maintain precision
    expected_mean = sum(data, Decimal('0')) / Decimal(len(data))
    assert result == expected_mean, f"Mean calculation incorrect: got {result}, expected {expected_mean}"


@given(st.lists(st.integers(), max_size=0))
def test_empty_data_error(empty_list):
    """Test that median raises StatisticsError for empty data sequences.
    
    This property-based test verifies that the median function properly raises
    a StatisticsError when given any empty sequence, which is the expected
    behavior according to the statistics module specification.
    """
    import pytest
    import statistics
    
    with pytest.raises(statistics.StatisticsError):
        statistics.median(empty_list)


@given(st.lists(st.integers(), min_size=1).filter(lambda x: len(x) % 2 == 1))
def test_odd_length_returns_middle_element(data):
    """Test that for odd length data, median returns the middle element.
    
    For a sorted list with odd length, the median should be exactly the element
    at the middle position (index len(data) // 2).
    """
    sorted_data = sorted(data)
    expected_median = sorted_data[len(sorted_data) // 2]
    result = median(data)
    assert result == expected_median


@given(st.lists(st.integers(min_value=-100, max_value=100), min_size=1), 
       st.integers(min_value=1, max_value=10))
def test_median_positive_scaling_property(data, k):
    """Test that median([k*x for x in data]) == k * median(data) for k > 0.
    
    This property holds because scaling all elements by a positive constant
    preserves their relative order, so the median element(s) scale by the 
    same factor. This is a fundamental property of the median function
    under positive scalar multiplication.
    
    Args:
        data: A non-empty list of integers to test
        k: A positive integer scaling factor (k > 0)
    """
    import math
    
    # Scale all elements by k
    scaled_data = [k * x for x in data]
    
    # Calculate median of scaled data
    result = median(scaled_data)
    
    # Calculate k times the median of original data
    expected = k * median(data)
    
    # Use math.isclose for robust floating-point comparison
    assert math.isclose(result, expected), f"median({scaled_data}) = {result}, but k * median({data}) = {expected}"


@given(st.just([]))
def test_empty_data_raises_statistics_error(data):
    """Test that empty data raises StatisticsError when calculating mode.
    
    Property being tested: The mode function should raise a StatisticsError
    with the message "no mode for empty data" when given an empty list.
    """
    with pytest.raises(StatisticsError, match="no mode for empty data"):
        mode(data)


@given(st.lists(st.integers(), min_size=1))
def test_returned_element_has_maximum_frequency(data):
    """Test that the returned element has the maximum frequency in the data.
    
    This test verifies that:
    1. The returned element actually appears in the input data
    2. The returned element has the maximum frequency among all elements
    3. The returned element is one of the valid mode candidates (handles ties correctly)
    """
    from collections import Counter
    
    result = mode(data)
    counter = Counter(data)
    max_frequency = max(counter.values())
    
    # Get all elements that have the maximum frequency (valid mode candidates)
    elements_with_max_freq = [elem for elem, freq in counter.items() if freq == max_frequency]
    
    # The result should be one of the elements with maximum frequency
    assert result in elements_with_max_freq, f"Result {result} is not among valid mode candidates {elements_with_max_freq}"
    
    # Also verify it actually has the maximum frequency
    assert counter[result] == max_frequency, f"Result {result} has frequency {counter[result]}, but max frequency is {max_frequency}"


@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e6, max_value=1e6), min_size=3, max_size=20))
def test_stdev_properties(data):
    """Test mathematical properties of standard deviation."""
    import math
    
    result = stdev(data)
    
    # Standard deviation should always be non-negative
    assert result >= 0
    
    # If all values are the same, standard deviation should be 0
    if len(set(data)) == 1:
        assert result == 0
    else:
        # If values differ, standard deviation should be positive
        assert result > 0
    
    # Standard deviation should be scale-invariant for multiplication
    # stdev([k*x for x in data]) should equal |k| * stdev(data)
    if result > 0:  # Only test scaling when there's variation in the data
        scaled_data = [2 * x for x in data]
        scaled_result = stdev(scaled_data)
        assert math.isclose(scaled_result, 2 * result, rel_tol=1e-9)
    
    # Standard deviation should be translation-invariant
    # stdev([x + c for x in data]) should equal stdev(data)
    translated_data = [x + 100 for x in data]
    translated_result = stdev(translated_data)
    assert math.isclose(translated_result, result, rel_tol=1e-9)


@given(
    st.floats(allow_nan=False, allow_infinity=False, min_value=-1000, max_value=1000),
    st.integers(min_value=2, max_value=10)
)
def test_stdev_zero_deviation_constant_data(c, n):
    """Test that stdev of constant data equals zero.
    
    Property: When all data points are identical, the standard deviation
    should be zero since there is no variation in the data.
    Uses n >= 2 to ensure valid standard deviation calculation.
    """
    import math
    data = [c] * n
    result = stdev(data)
    # Use a more reasonable tolerance for floating-point comparisons
    # to account for potential precision errors in calculations
    assert math.isclose(result, 0, abs_tol=1e-12)


@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-100, max_value=100), min_size=2, max_size=10))
def test_stdev_relationship_to_variance(data):
    """
    Test that stdev(data) == sqrt(variance(data)).
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance. Uses relaxed tolerance to account for
    floating-point precision errors that can accumulate through square root
    operations.
    """
    import math
    stdev_result = stdev(data)
    variance_result = variance(data)
    expected = math.sqrt(variance_result)
    
    # Use more reasonable tolerances to handle floating-point precision errors
    # rel_tol=1e-12 for relative tolerance and abs_tol=1e-15 for cases near zero
    assert math.isclose(stdev_result, expected, rel_tol=1e-12, abs_tol=1e-15), \
        f"stdev({data}) = {stdev_result}, but sqrt(variance({data})) = {expected}"


@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=50))
def test_variance_non_negative(data):
    """Test that variance is always non-negative.
    
    The variance of any dataset should always be >= 0, as it represents
    the average of squared deviations from the mean. We constrain the
    input values to a reasonable range (-1e10 to 1e10) to avoid
    numerical overflow during variance calculations while still
    testing the fundamental mathematical property.
    """
    result = variance(data)
    assert result >= 0, f"Variance should be non-negative, but got {result} for data: {data}"


@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=50),
       st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10))
def test_variance_translation_invariance(data, translation):
    """Test that variance is unchanged by adding a constant to all data points.
    
    This tests the mathematical property that Var(X + c) = Var(X) for any constant c.
    The variance should be translation invariant because adding a constant shifts
    all values equally, which doesn't change the spread of the data.
    
    We limit the range of floats to avoid numerical precision issues that can occur
    with extremely large floating-point numbers during variance calculations.
    """
    import math
    original_variance = variance(data)
    translated_data = [x + translation for x in data]
    translated_variance = variance(translated_data)
    
    # Use a more lenient relative tolerance since we're dealing with floating-point arithmetic
    # and variance calculations involve squared differences which can amplify small errors
    assert math.isclose(original_variance, translated_variance, rel_tol=1e-9, abs_tol=1e-15)


@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=100))
def test_variance_with_explicit_mean_equals_variance_with_none(data):
    """
    Test that variance(data, mean(data)) == variance(data, None).
    
    This property verifies that explicitly passing the mean as the xbar parameter
    produces identical results to letting variance calculate the mean internally.
    The variance function should behave identically whether the mean is provided
    or computed automatically.
    
    Note: We restrict the floating-point range to [-1e10, 1e10] to avoid
    numerical instability issues that can occur with extremely large numbers,
    which would cause legitimate differences in floating-point arithmetic
    depending on the computational path taken.
    """
    import math
    
    # Calculate the mean of the data
    computed_mean = mean(data)
    
    # Get variance with explicit mean
    variance_with_mean = variance(data, computed_mean)
    
    # Get variance with None (auto-calculated mean)
    variance_with_none = variance(data, None)
    
    # They should be equal (using close comparison for floating point)
    # Using a slightly more lenient tolerance to account for expected
    # floating-point precision differences in the computation paths
    assert math.isclose(variance_with_mean, variance_with_none, rel_tol=1e-12)


@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=1e100), min_size=2, max_size=100))
def test_stdev_with_explicit_mean_equals_stdev_with_none(data):
    """
    Test that stdev(data, mean(data)) == stdev(data, None).
    
    This property tests that explicitly passing the mean as the xbar parameter
    produces the same result as letting stdev calculate it internally (None).
    
    The input is constrained to avoid extremely large values that cause numerical
    instability in floating point arithmetic, and uses a reasonable tolerance
    for comparison to account for precision differences between the two approaches.
    """
    import math
    from statistics import stdev, mean
    
    # Calculate stdev with explicit mean
    data_mean = mean(data)
    stdev_with_mean = stdev(data, data_mean)
    
    # Calculate stdev with None (let it calculate mean internally)
    stdev_with_none = stdev(data, None)
    
    # Compare using math.isclose with reasonable tolerance for floating point comparison
    # Use rel_tol=1e-9 to account for numerical instability with large values
    assert math.isclose(stdev_with_mean, stdev_with_none, rel_tol=1e-9), \
        f"stdev with explicit mean {stdev_with_mean} != stdev with None {stdev_with_none}"


@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), 
       st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10))
def test_variance_equals_stdev_squared_for_two_elements(a, b):
    """
    Test that for two-element data [a, b]: variance([a, b]) == stdev([a, b])^2
    
    This verifies the fundamental relationship between variance and standard deviation
    (stdev = sqrt(variance)) for the minimal case where both functions are defined
    (requiring at least 2 data points).
    
    Note: Float range is limited to avoid extreme values that cause numerical
    precision issues in floating point arithmetic.
    """
    import math
    from statistics import variance, stdev
    
    data = [a, b]
    
    # Calculate variance and standard deviation
    var_result = variance(data)
    stdev_result = stdev(data)
    
    # Test the fundamental relationship: variance = stdev^2
    stdev_squared = stdev_result ** 2
    
    # Use math.isclose with appropriate tolerance for floating point comparison
    # rel_tol=1e-9 handles relative precision errors for larger values
    # abs_tol=1e-12 handles absolute precision errors for values close to zero
    assert math.isclose(var_result, stdev_squared, rel_tol=1e-9, abs_tol=1e-12), \
        f"variance([{a}, {b}]) = {var_result} != {stdev_squared} = stdev([{a}, {b}])^2"

@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1000, max_value=1000))
def test_stdev_single_duplication_invariance(x):
    """Test that stdev([x, x]) == 0.
    
    When all values in a dataset are identical, the standard deviation should be 0.
    This tests the mathematical property that there is no variation when all data points
    are the same. Uses a relaxed tolerance to account for floating-point precision errors.
    """
    import math
    data = [x, x]
    result = stdev(data)
    assert math.isclose(result, 0, abs_tol=1e-12)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=1e100), min_size=1), 
       st.floats(allow_nan=False, allow_infinity=False, min_value=-1e6, max_value=1e6))
def test_mean_linear_scaling(data, k):
    """Test that mean([k*x for x in data]) == k * mean(data)
    
    This property tests the linearity of the mean function under scalar multiplication.
    Special handling is needed for k=0 due to floating-point representation issues,
    and adaptive tolerance is used to account for accumulated precision errors.
    
    The input ranges are restricted to avoid extreme floating-point values that
    would cause overflow or severe precision loss when scaled.
    """
    import math
    
    scaled_data = [k * x for x in data]
    original_mean = mean(data)
    scaled_mean = mean(scaled_data)
    expected = k * original_mean
    
    # Handle the special case where k=0
    # When k=0, all scaled_data elements are 0, so mean should be exactly 0.0
    if k == 0:
        assert scaled_mean == 0.0
    else:
        # Use adaptive tolerance that accounts for accumulated errors
        # scaling with the number of operations rather than magnitude
        # This avoids issues with extremely large values
        rel_tolerance = max(1e-12, 1e-13 * len(data))
        
        # Use both relative and absolute tolerance for robustness
        # Absolute tolerance handles cases where expected value is very small
        abs_tolerance = 1e-15
        
        assert math.isclose(scaled_mean, expected, rel_tol=rel_tolerance, abs_tol=abs_tolerance), \
            f"scaled_mean={scaled_mean}, expected={expected}, diff={abs(scaled_mean - expected)}, " \
            f"data_size={len(data)}, k={k}"

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1),
       st.floats(allow_nan=False, allow_infinity=False, min_value=-1e3, max_value=1e3))
def test_mean_translation_invariant(data, c):
    """Test that mean([x+c for x in data]) == mean(data) + c
    
    This property tests translation invariance: adding a constant to all values
    should shift the mean by that same constant. Uses a reduced range for c
    and appropriate tolerance to handle floating-point precision issues.
    """
    import math
    translated_data = [x + c for x in data]
    original_mean = mean(data)
    translated_mean = mean(translated_data)
    expected = original_mean + c
    assert math.isclose(translated_mean, expected, rel_tol=1e-9, abs_tol=1e-9)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, max_value=1e50, min_value=-1e50), min_size=1),
       st.lists(st.floats(allow_nan=False, allow_infinity=False, max_value=1e50, min_value=-1e50), min_size=1))
def test_mean_concatenation_property(a, b):
    """
    Test that mean(a + b) == (len(a)*mean(a) + len(b)*mean(b)) / (len(a) + len(b))
    
    This property tests the linearity of the mean function over concatenated lists.
    The mean of concatenated lists should equal the weighted average of the individual means,
    where weights are the lengths of the respective lists.
    
    We constrain float values to avoid extreme numbers that cause precision issues,
    and use a relaxed tolerance to account for floating point arithmetic limitations.
    """
    import math
    from statistics import mean  # Import the mean function being tested
    
    concatenated = a + b
    concatenated_mean = mean(concatenated)
    
    mean_a = mean(a)
    mean_b = mean(b)
    expected = (len(a) * mean_a + len(b) * mean_b) / (len(a) + len(b))
    
    # Use more relaxed tolerance to handle floating point precision issues
    # especially when dealing with numbers of different magnitudes
    assert math.isclose(concatenated_mean, expected, rel_tol=1e-9, abs_tol=1e-12)

import math

@given(st.lists(st.integers(), min_size=1))
def test_mean_integer_input(data):
    """Test that mean of integers returns int or float and is mathematically correct"""
    result = mean(data)
    assert isinstance(result, (int, float))
    # Verify correctness with floating-point tolerance to handle precision issues
    # that occur with very large integers exceeding float precision limits
    expected = sum(data) / len(data)
    assert math.isclose(result, expected, rel_tol=1e-9, abs_tol=1e-9)

@given(st.lists(st.one_of(st.integers(), st.floats(allow_nan=False, allow_infinity=False)), min_size=1))
def test_mean_mixed_type_input(data):
    """Test that mean of mixed int/float data returns correct result"""
    result = mean(data)
    assert isinstance(result, (int, float))
    # Verify correctness with tolerance for floating point arithmetic
    expected = sum(data) / len(data)
    # Always use isclose for comparison since mixed types typically produce floats
    assert math.isclose(result, expected, rel_tol=1e-9)

@given(st.lists(st.fractions(), min_size=1))
def test_mean_fraction_type_preservation(data):
    """Test that mean preserves Fraction type and computes correct mathematical mean"""
    from fractions import Fraction
    
    result = mean(data)
    
    # Test type preservation: result should be a Fraction
    assert isinstance(result, Fraction)
    
    # Test mathematical correctness: verify the mean is calculated correctly
    # Use Fraction division to preserve type and ensure precise comparison
    expected = sum(data) / Fraction(len(data))
    assert result == expected

@given(st.lists(st.integers(), min_size=1), st.randoms())
def test_order_invariance(data, rnd):
    """Test that median is invariant to the order of input data.
    
    The median of a dataset should be the same regardless of how the
    elements are ordered. This test verifies that the median function
    produces identical results for the original data, sorted data,
    reverse-sorted data, and randomly shuffled data.
    """
    # Calculate median of original data as the reference
    original_median = median(data)
    
    # Test with sorted data - should give same result
    sorted_data = sorted(data)
    assert median(sorted_data) == original_median, "Median should be invariant to sorting"
    
    # Test with reverse sorted data - should give same result
    reverse_sorted_data = sorted(data, reverse=True)
    assert median(reverse_sorted_data) == original_median, "Median should be invariant to reverse sorting"
    
    # Test with shuffled data using Hypothesis's deterministic random state
    shuffled_data = data.copy()
    rnd.shuffle(shuffled_data)
    assert median(shuffled_data) == original_median, "Median should be invariant to shuffling"

@given(st.integers(min_value=1, max_value=25).flatmap(lambda n: st.lists(st.integers(min_value=-1000, max_value=1000), min_size=2*n, max_size=2*n)))
def test_even_length_median_interpolation(data):
    """Test that for even length data, median follows the standard interpolation rule (average of two middle elements)."""
    import math
    sorted_data = sorted(data)
    n = len(sorted_data)
    assert n % 2 == 0, "This test is specifically for even-length lists"
    
    # Standard definition: median of even-length list is average of two middle elements
    middle_left = sorted_data[n//2 - 1]
    middle_right = sorted_data[n//2]
    expected = (middle_left + middle_right) / 2
    
    result = median(data)
    assert math.isclose(result, expected, rel_tol=1e-12, abs_tol=1e-12)

@given(st.integers(min_value=-(2**53 - 1), max_value=2**53 - 1), st.integers(min_value=1, max_value=20))
def test_duplicate_handling(x, count):
    """Test that median of all identical elements returns that element.
    
    This property verifies that when all elements in a list are identical,
    the median function correctly returns that element. The integer range
    is restricted to values that can be exactly represented as floats
    (within 2^53 - 1) to avoid floating-point precision issues.
    
    Edge cases covered:
    - Single element lists (count=1)
    - Lists with multiple identical elements
    - Large integers within safe float representation range
    """
    data = [x] * count
    assert median(data) == x

@given(st.lists(st.integers(min_value=-10**6, max_value=10**6), min_size=1, max_size=20))
def test_median_properties(numbers):
    """Test fundamental properties of median function.
    
    This comprehensive test verifies that the median function correctly computes
    the median for lists of various sizes by testing:
    1. For odd-length lists: median equals the middle element when sorted
    2. For even-length lists: median equals average of two middle elements when sorted
    3. Median is invariant under permutation of input
    4. Result is mathematically correct for the general case
    
    Using bounded integers and limited list size to avoid performance issues
    while still providing comprehensive coverage.
    """
    import math
    import random
    
    result = median(numbers)
    sorted_nums = sorted(numbers)
    n = len(sorted_nums)
    
    # Test that median matches mathematical definition
    if n % 2 == 1:
        # Odd length: median is the middle element
        expected = sorted_nums[n // 2]
        assert result == expected, f"For odd-length list {sorted_nums}, expected {expected}, got {result}"
    else:
        # Even length: median is average of two middle elements
        expected = (sorted_nums[n // 2 - 1] + sorted_nums[n // 2]) / 2
        assert math.isclose(result, expected), f"For even-length list {sorted_nums}, expected {expected}, got {result}"
    
    # Test that median is invariant under permutation
    shuffled = numbers.copy()
    random.shuffle(shuffled)
    shuffled_result = median(shuffled)
    assert math.isclose(median(shuffled), result), f"Median should be invariant under permutation: original={result}, shuffled={shuffled_result}"


@given(st.integers(min_value=-10**6, max_value=10**6))
def test_single_element_median(x):
    """Test that median of single element list is the element itself."""
    result = median([x])
    assert result == x, f"Median of single element [{x}] should be {x}, got {result}"


@given(st.integers(min_value=-10**6, max_value=10**6), st.integers(min_value=-10**6, max_value=10**6))
def test_two_element_median(a, b):
    """Test that median of two elements is their average.
    
    For any two numbers, the median equals their arithmetic mean.
    This is a specific case but important to test explicitly.
    """
    import math
    result = median([a, b])
    expected = (a + b) / 2
    assert math.isclose(result, expected), f"Median of [{a}, {b}] should be {expected}, got {result}"

@given(st.lists(st.integers(), min_size=1, max_size=10).flatmap(
    lambda data1: st.tuples(
        st.just(data1),
        st.lists(st.integers(), min_size=len(data1), max_size=len(data1))
            .map(lambda offsets: [x + abs(offset) for x, offset in zip(data1, offsets)])
    )
))
def test_monotonicity_preservation(data_pair):
    """Test that if all elements in data1 ≤ corresponding elements in data2, then median(data1) ≤ median(data2)."""
    data1, data2 = data_pair
    
    # Verify precondition: all elements in data1 are ≤ corresponding elements in data2
    assert all(x <= y for x, y in zip(data1, data2))
    
    # Test the monotonicity property: median(data1) ≤ median(data2)
    assert median(data1) <= median(data2)

@given(st.lists(st.integers(), min_size=1))
def test_return_value_is_mode_of_input_data_integers(data):
    """Test that the returned mode is the most frequent element from the input data.
    
    When there are multiple elements tied for the highest frequency (multiple modes),
    the function should return the one that appears first in the input data.
    This test verifies both the frequency requirement and the tie-breaking behavior.
    """
    result = mode(data)
    
    # Verify the result is in the input data
    assert result in data
    
    # Verify the result is actually the mode (most frequent element)
    from collections import Counter
    counter = Counter(data)
    max_count = max(counter.values())
    
    # Verify the result has the maximum count
    assert counter[result] == max_count
    
    # Verify tie-breaking behavior: should return the first occurrence among ties
    most_frequent_elements = [elem for elem, count in counter.items() if count == max_count]
    
    if len(most_frequent_elements) > 1:
        # When there are ties, find which element appears first in the original data
        first_occurrences = {elem: data.index(elem) for elem in most_frequent_elements}
        expected_result = min(first_occurrences, key=first_occurrences.get)
        assert result == expected_result, (
            f"Expected mode() to return the first occurring element among ties. "
            f"Got {result}, but {expected_result} appears first at index {data.index(expected_result)}"
        )

from collections import Counter

@given(st.lists(st.text(), min_size=1))
def test_return_value_is_element_from_input_data_strings(data):
    """Test that the returned mode is one of the most frequent elements from the input data (strings)."""
    result = mode(data)
    
    # First verify the result exists in the input data
    assert result in data
    
    # Verify that the result is actually a mode (most frequent element)
    counter = Counter(data)
    max_count = max(counter.values())
    modes = [item for item, count in counter.items() if count == max_count]
    assert result in modes, f"Result '{result}' is not one of the modes {modes}"
    assert counter[result] == max_count, f"Result '{result}' has count {counter[result]}, but max count is {max_count}"

@given(st.text())
def test_single_element_list_returns_that_element(single_element):
    """Test that mode of a single-element list returns that element."""
    data = [single_element]
    result = mode(data)
    assert result == single_element

@given(st.text(min_size=1), st.text(min_size=1))
def test_first_occurrence_wins_for_ties(first_element, second_element):
    """Test that when multiple elements have the same maximum frequency, the first one encountered wins."""
    # Filter to ensure we have two distinct elements
    assume(first_element != second_element)
    
    # Create a scenario where we know there will be ties
    # Both elements appear with the same maximum frequency (2 times each)
    # first_element appears first in the list, so it should win the tie
    test_data = [first_element, second_element, first_element, second_element]
    
    result = mode(test_data)
    # The first element should win the tie since it appeared first
    assert result == first_element, f"Expected {first_element} to win tie, but got {result}"

@given(st.lists(st.integers(), min_size=1))
def test_works_with_integers(data):
    """Test that mode works with integer data and returns actual mode(s).
    
    This test verifies that:
    1. The mode function returns integer(s) that exist in the input data
    2. The returned value(s) are actually the most frequent element(s)
    3. The function handles both single and multiple mode cases correctly
    4. No duplicates are returned in the result
    5. The result is non-empty
    """
    from collections import Counter
    
    result = mode(data)
    
    # Normalize result to always be a list for consistent testing
    # This handles both cases: when mode() returns a single value or a list
    if isinstance(result, list):
        modes = result
    else:
        modes = [result]
    
    # Ensure the result is non-empty
    assert len(modes) > 0, "Mode function returned empty result"
    
    # All returned modes should be integers from the input data
    for m in modes:
        assert isinstance(m, int), f"Mode {m} is not an integer"
        assert m in data, f"Mode {m} is not present in the input data {data}"
    
    # Calculate the expected modes using Counter
    counts = Counter(data)
    max_count = max(counts.values())
    expected_modes = [val for val, count in counts.items() if count == max_count]
    
    # The returned modes should exactly match the mathematically correct modes
    assert set(modes) == set(expected_modes), \
        f"Expected modes {sorted(expected_modes)}, but got {sorted(modes)} for data {data}"
    
    # Ensure no duplicates in the returned modes
    assert len(modes) == len(set(modes)), \
        f"Duplicate modes returned: {modes} for data {data}"
    
    # Verify that each mode actually has the maximum frequency
    for m in modes:
        assert counts[m] == max_count, \
            f"Mode {m} has frequency {counts[m]}, but max frequency is {max_count}"

@given(st.lists(st.text(min_size=1), min_size=1))
def test_works_with_strings(data):
    """Test that mode works with string data and returns the most frequent element.
    
    This test verifies that:
    1. The mode function returns a string when given string data
    2. The returned value is present in the original data
    3. The returned value has the maximum frequency count (is actually a mode)
    4. Handles ties appropriately (returns one of the valid modes)
    """
    result = mode(data)
    assert isinstance(result, str)
    assert result in data
    
    # Count frequencies to verify the result is actually a mode
    from collections import Counter
    counts = Counter(data)
    max_count = max(counts.values())
    
    # The result should be one of the elements with maximum frequency
    assert counts[result] == max_count

@given(st.lists(st.tuples(st.integers(), st.text()), min_size=1))
def test_works_with_tuples(data):
    """Test that mode works with tuple data (hashable) and returns the most frequent element."""
    result = mode(data)
    
    # Verify the result is a tuple
    assert isinstance(result, tuple)
    
    # Count frequencies to verify it's actually the mode
    from collections import Counter
    counts = Counter(data)
    max_count = max(counts.values())
    
    # The returned tuple must have the maximum frequency count
    assert counts[result] == max_count, f"Result {result} appears {counts[result]} times, but max frequency is {max_count}"

@given(st.lists(st.integers(), min_size=1))
def test_mode_returns_all_most_frequent_elements(data):
    """Test that the mode function returns all most frequently occurring elements as a list.
    
    The mode function should return a list containing exactly all elements that appear
    with the maximum frequency in the input data. For example:
    - [1, 2, 2, 3] should return [2]
    - [1, 1, 2, 2, 3] should return [1, 2] (in any order)
    - [5] should return [5]
    """
    result = mode(data)
    
    # Count frequencies to determine expected modes
    from collections import Counter
    counts = Counter(data)
    max_count = max(counts.values())
    expected_modes = sorted([item for item, count in counts.items() if count == max_count])
    
    # The result should be a list containing exactly all most frequent elements
    assert isinstance(result, list), "Mode function should return a list"
    assert len(result) > 0, "Mode function should return at least one mode"
    assert sorted(result) == expected_modes, f"Should return exactly all most frequent elements. Expected {expected_modes}, got {sorted(result)}"
    
    # Verify that all returned elements actually have the maximum frequency
    for element in result:
        assert counts[element] == max_count, f"Element {element} should have maximum frequency {max_count}"
    
    # Verify no duplicates in the result
    assert len(result) == len(set(result)), "Result should not contain duplicate modes"

@given(st.lists(st.text(alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd')), min_size=1, max_size=10), min_size=2, max_size=20))
def test_order_preservation_for_tie_breaking(data):
    """Test that mode preserves order for tie-breaking - first occurrence wins among tied elements.
    
    This property-based test verifies that when multiple elements have the same highest frequency,
    the mode function returns the element that appears first in the original list.
    
    The test uses alphanumeric strings (uppercase letters, lowercase letters, digits) to avoid
    edge cases with whitespace-only strings or special characters that might cause unexpected
    behavior in the mode function.
    """
    result = mode(data)
    
    # Count frequencies and track first occurrence position for each element
    counts = {}
    first_occurrence = {}
    for i, item in enumerate(data):
        if item not in counts:
            counts[item] = 0
            first_occurrence[item] = i
        counts[item] += 1
    
    # Find the maximum frequency
    max_freq = max(counts.values())
    
    # Find all elements that have the maximum frequency (tied for mode)
    tied_items = [item for item, count in counts.items() if count == max_freq]
    
    # The result should be one of the tied items
    assert result in tied_items, f"Result {result} is not among the most frequent items {tied_items}"
    
    # If there are multiple tied items, the result should be the one that appears first
    if len(tied_items) > 1:
        expected_winner = min(tied_items, key=lambda x: first_occurrence[x])
        assert result == expected_winner, (
            f"Expected {expected_winner} (first occurrence at index {first_occurrence[expected_winner]}) "
            f"to win tie-breaking, but got {result} (first occurrence at index {first_occurrence[result]})"
        )
    
    # Verify that the result actually appears in the original data
    assert result in data, f"Result {result} does not appear in the input data {data}"
    
    # Verify that the result has the maximum frequency
    assert counts[result] == max_freq, (
        f"Result {result} has frequency {counts[result]}, but maximum frequency is {max_freq}"
    )

@given(st.lists(st.integers(min_value=1, max_value=5), min_size=2, max_size=8))
def test_mode_with_tie_scenario(data):
    """Test mode when multiple elements have the same highest frequency (tie scenario)."""
    # Create a tie scenario: duplicate each element exactly twice
    tied_data = [x for x in data for _ in range(2)]
    
    result = mode(tied_data)
    
    # In a tie scenario, result should be one of the tied elements
    assert result in data, f"Mode {result} should be one of the tied elements {set(data)}"
    
    # Verify that the returned element actually has the maximum frequency
    from collections import Counter
    counts = Counter(tied_data)
    max_count = max(counts.values())
    assert counts[result] == max_count, f"Mode {result} should have max frequency {max_count}"

@given(st.integers(min_value=1, max_value=10), st.integers(min_value=2, max_value=5))
def test_mode_with_multiple_ties(base_value, num_tied_elements):
    """Test mode when multiple distinct elements are tied for highest frequency."""
    # Create multiple elements each appearing exactly 3 times
    tied_elements = list(range(base_value, base_value + num_tied_elements))
    tied_data = [x for x in tied_elements for _ in range(3)]
    
    result = mode(tied_data)
    
    # Result should be one of the tied elements
    assert result in tied_elements, f"Mode {result} should be one of {tied_elements}"
    
    # Verify that the returned element actually has the maximum frequency
    from collections import Counter
    counts = Counter(tied_data)
    max_count = max(counts.values())
    assert counts[result] == max_count, f"Mode {result} should have max frequency {max_count}"

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0, max_size=1))
def test_stdev_error_condition_insufficient_data(data):
    """Test that stdev raises StatisticsError when given fewer than 2 data points.
    
    Standard deviation requires at least 2 data points to calculate the variance
    from the mean. Both empty lists and single-element lists should raise
    StatisticsError since there's insufficient data to compute a meaningful
    standard deviation.
    """
    import pytest
    from statistics import StatisticsError, stdev
    
    with pytest.raises(StatisticsError):
        stdev(data)

@given(
    st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e6, max_value=1e6), min_size=2, max_size=10),
    st.floats(allow_nan=False, allow_infinity=False, min_value=-100, max_value=100).filter(lambda x: x != 0)
)
def test_stdev_scale_invariance(data, k):
    """Test that stdev([k*x for x in data]) == abs(k) * stdev(data) for k != 0.
    
    This tests the mathematical property that standard deviation scales linearly
    with the absolute value of the scaling factor. This is a fundamental property
    of standard deviation that should hold for any non-zero scaling factor k.
    
    Edge cases handled:
    - Constant data (original_stdev == 0): scaled data should also have stdev == 0
    - Large scaling factors: uses appropriate tolerance for floating-point precision
    """
    import math
    original_stdev = stdev(data)
    scaled_data = [k * x for x in data]
    scaled_stdev = stdev(scaled_data)
    expected = abs(k) * original_stdev
    
    # Handle the case where original data has zero standard deviation (constant data)
    if original_stdev == 0:
        assert scaled_stdev == 0, f"Scaled constant data should have zero stdev, got {scaled_stdev}"
    else:
        # Use a more lenient tolerance to account for floating-point precision issues
        # that can accumulate through multiplication and standard deviation calculations.
        # The tolerance accounts for:
        # 1. Precision loss when multiplying each data point by k
        # 2. Accumulated errors in variance and square root calculations
        # 3. Larger absolute errors when k has large magnitude
        assert math.isclose(scaled_stdev, expected, rel_tol=1e-8), \
            f"Scale invariance failed: scaled_stdev={scaled_stdev}, expected={expected}, k={k}"

@given(
    st.lists(
        st.floats(
            allow_nan=False, 
            allow_infinity=False, 
            min_value=-1e3, 
            max_value=1e3
        ).filter(lambda x: abs(x) >= 1e-10),  # Exclude effectively zero values
        min_size=2, 
        max_size=10
    ).filter(lambda data: len(set(data)) > 1),  # Ensure variance exists (not all identical)
    st.floats(
        allow_nan=False, 
        allow_infinity=False, 
        min_value=-100, 
        max_value=100
    )
)
def test_stdev_translation_invariance(data, c):
    """Test that stdev([x + c for x in data]) == stdev(data).
    
    Translation invariance is a fundamental property of standard deviation:
    adding a constant to all values should not change the spread (standard deviation).
    This test verifies this mathematical property by ensuring the standard deviation
    remains the same after translating all data points by a constant value.
    
    The strategy filters out effectively zero values and ensures variance exists
    by requiring at least two distinct values in the dataset.
    """
    import math
    
    original_stdev = stdev(data)
    translated_data = [x + c for x in data]
    translated_stdev = stdev(translated_data)
    
    # Use both relative and absolute tolerance to handle floating-point precision
    assert math.isclose(translated_stdev, original_stdev, rel_tol=1e-9, abs_tol=1e-12), \
        f"Translation invariance failed: original_stdev={original_stdev}, translated_stdev={translated_stdev}, data={data}, c={c}"

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1000, max_value=1000), min_size=2, max_size=10))
def test_stdev_consistency_with_xbar_parameter(data):
    """Test that stdev(data, xbar) == stdev(data) when xbar equals the actual mean.
    
    This property verifies that providing the correct mean as the xbar parameter
    should produce the same result as letting stdev calculate the mean internally.
    The test uses appropriate tolerances to account for floating-point precision
    issues in numerical computations.
    """
    import math
    mean_value = sum(data) / len(data)
    stdev_without_xbar = stdev(data)
    stdev_with_xbar = stdev(data, xbar=mean_value)
    
    # Use more reasonable tolerances for floating-point comparison
    # Handle the case where stdev is 0 (all values identical) by using absolute tolerance
    if stdev_without_xbar == 0:
        assert stdev_with_xbar == 0
    else:
        assert math.isclose(stdev_without_xbar, stdev_with_xbar, rel_tol=1e-12, abs_tol=1e-12)

import pytest
from statistics import StatisticsError, variance

@given(st.just([]))
def test_variance_error_empty_data(data):
    """Test that variance raises StatisticsError when given empty data."""
    with pytest.raises(StatisticsError):
        variance(data)

@given(st.floats(allow_nan=False, allow_infinity=False))
def test_variance_single_element(value):
    """Test that variance returns 0.0 for single element data (no variation)."""
    data = [value]
    result = variance(data)
    assert result == 0.0

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=10))
def test_variance_sufficient_data(data):
    """Test that variance works correctly when given sufficient data (2 or more elements).
    
    This test verifies that:
    1. The variance function returns a numeric result for valid input
    2. The variance is always non-negative (fundamental mathematical property)
    3. The function handles floating-point numbers without overflow
    
    The float range is restricted to [-1e10, 1e10] to avoid numerical overflow
    issues that can occur with extremely large values during variance computation.
    """
    # This should not raise an exception and should return a non-negative float
    result = variance(data)
    assert isinstance(result, (int, float))
    assert result >= 0  # Variance is always non-negative

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=1e100), min_size=2, max_size=50))
def test_variance_mean_consistency(data):
    """Test that variance with explicit mean equals variance with computed mean.
    
    This property verifies that calculating variance by letting the function
    compute the mean internally produces the same result as explicitly passing
    the mean as a parameter. Uses constrained float values to avoid numerical
    instability from extremely large numbers, and relaxed tolerances to account 
    for floating-point precision issues during intermediate variance calculations.
    """
    import math
    from statistics import mean
    computed_variance = variance(data)
    explicit_mean_variance = variance(data, mean(data))
    assert math.isclose(computed_variance, explicit_mean_variance, rel_tol=1e-6, abs_tol=1e-8)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e6, max_value=1e6), min_size=2, max_size=20),
       st.floats(allow_nan=False, allow_infinity=False, min_value=-10, max_value=10).filter(lambda x: x != 0))
def test_variance_scale_invariance(data, scale_factor):
    """Test that variance scales by the square of the scaling factor.
    
    This property test verifies that when all data points are multiplied by a constant
    scale factor, the variance of the scaled data equals the original variance multiplied
    by the square of the scale factor: Var(c*X) = c²*Var(X).
    
    Handles the edge case where original variance is zero (all data points identical)
    by checking that scaled variance is also zero. For non-zero variance, uses relative
    tolerance to account for floating-point precision errors.
    """
    import math
    original_variance = variance(data)
    scaled_data = [scale_factor * x for x in data]
    scaled_variance = variance(scaled_data)
    expected_variance = (scale_factor ** 2) * original_variance
    
    # Handle the case where original variance is zero (all data points identical)
    if original_variance == 0:
        assert scaled_variance == 0
    else:
        assert math.isclose(scaled_variance, expected_variance, rel_tol=1e-6)

@given(st.lists(st.integers(min_value=-100, max_value=100), min_size=2, max_size=20))
def test_variance_mathematical_property_integers(data):
    """Test that variance calculation is mathematically correct for integers."""
    result = variance(data)
    
    # Variance should be non-negative
    assert result >= 0
    
    # Calculate expected variance manually using the sample variance formula (n-1)
    # This is the standard formula used by most statistical libraries
    mean = sum(data) / len(data)
    expected_variance = sum((x - mean) ** 2 for x in data) / (len(data) - 1)
    
    # Compare with tolerance for floating point arithmetic
    assert abs(result - expected_variance) < 1e-10
    
    # Result should be numeric (variance of integers can be float due to division)
    assert isinstance(result, (int, float))
    
    # Variance should be zero if and only if all elements are equal
    all_equal = len(set(data)) == 1
    if all_equal:
        assert result == 0
    else:
        assert result > 0

@given(st.lists(st.fractions(min_value=-10, max_value=10), min_size=2, max_size=10))
def test_variance_fractions(data):
    """Test that variance works correctly with Fraction types and satisfies mathematical properties."""
    from fractions import Fraction
    import math
    
    result = variance(data)
    
    # Test that the function doesn't crash and returns a numeric result
    assert isinstance(result, (int, float, Fraction))
    
    # Test basic mathematical properties of variance
    # Variance should be non-negative
    assert result >= 0
    
    # If all values are the same, variance should be 0
    if len(set(data)) == 1:
        assert result == 0
    
    # Variance should be 0 if and only if all values are identical
    if result == 0:
        assert len(set(data)) == 1, "Variance is 0 only when all values are identical"
    
    # Test that variance is scale-invariant in terms of ordering
    # (variance of scaled data should be scaled by the square of the scaling factor)
    # But we'll test a simpler property: adding a constant to all values shouldn't change variance
    constant = Fraction(5, 1)
    shifted_data = [x + constant for x in data]
    shifted_result = variance(shifted_data)
    
    # Convert to float for comparison if needed to handle mixed types
    if isinstance(result, Fraction) and isinstance(shifted_result, Fraction):
        assert result == shifted_result, "Adding constant to all values should not change variance"
    else:
        assert math.isclose(float(result), float(shifted_result), rel_tol=1e-10), \
            "Adding constant to all values should not change variance"

@given(st.fractions(min_value=-10, max_value=10), st.integers(min_value=2, max_value=10))
def test_variance_identical_values_is_zero(value, size):
    """Test variance of identical values (same Fraction) is zero."""
    from fractions import Fraction
    data = [value] * size
    result = variance(data)
    assert isinstance(result, Fraction)
    assert result == Fraction(0)

@given(st.fractions(min_value=-10, max_value=10).filter(lambda x: x != 0), 
       st.integers(min_value=2, max_value=10))
def test_variance_different_values_positive(base_value, size):
    """Test variance of different values is positive."""
    from fractions import Fraction
    data = [base_value * i for i in range(1, size + 1)]
    result = variance(data)
    assert isinstance(result, Fraction)
    assert result > Fraction(0)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=1e100), min_size=2, max_size=20))
def test_variance_order_independence(data):
    """Test that variance is independent of data order.
    
    This property test verifies that the variance calculation produces the same result
    regardless of the order of elements in the input data. We test multiple deterministic
    orderings (original, sorted, reversed) to ensure order independence without relying
    on random operations that could make the test non-deterministic.
    """
    import math
    
    original_variance = variance(data)
    
    # Test with sorted order - this creates a completely different ordering
    # for most inputs while being deterministic
    sorted_variance = variance(sorted(data))
    assert math.isclose(original_variance, sorted_variance, rel_tol=1e-9), \
        f"Variance should be order-independent: original={original_variance}, sorted={sorted_variance}"
    
    # Test with reversed order - provides another deterministic permutation
    reversed_variance = variance(list(reversed(data)))
    assert math.isclose(original_variance, reversed_variance, rel_tol=1e-9), \
        f"Variance should be order-independent: original={original_variance}, reversed={reversed_variance}"
    
    # Test with reverse-sorted order for even more thorough testing
    reverse_sorted_variance = variance(sorted(data, reverse=True))
    assert math.isclose(original_variance, reverse_sorted_variance, rel_tol=1e-9), \
        f"Variance should be order-independent: original={original_variance}, reverse_sorted={reverse_sorted_variance}"

import math
import statistics

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=1e100), min_size=2, max_size=25))
def test_standard_deviation_duplicate_value_handling(data):
    """Test that standard deviation handles duplicated datasets appropriately.
    
    The key property being tested is that duplicating all values in a dataset
    should not change the standard deviation, since standard deviation measures 
    the spread of data around the mean regardless of sample size. When we 
    duplicate data, we get the same mean and the same spread, so standard 
    deviation should remain unchanged.
    
    This is mathematically correct because standard deviation measures the 
    typical distance from the mean, which doesn't change when we duplicate 
    the entire dataset.
    """
    original_std = statistics.stdev(data)
    duplicated_data = data + data
    duplicated_std = statistics.stdev(duplicated_data)
    
    # Both should be non-negative (standard deviation is always >= 0)
    assert original_std >= 0
    assert duplicated_std >= 0
    
    # Key property: duplicating data should not change the standard deviation
    # because standard deviation measures spread, not sample size
    assert math.isclose(original_std, duplicated_std, rel_tol=1e-10)

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=20))
def test_variance_minimum_bound_equality_condition(data):
    """Test that variance equals zero if and only if all elements are equal.
    
    This property tests the fundamental mathematical property that variance is zero
    when all values in the dataset are identical. Due to floating-point precision
    limitations, we use tolerance-based comparisons for both the variance check
    and the equality check to ensure consistent behavior.
    """
    import math
    
    result = variance(data)
    
    # Use tolerance-based comparison to determine if all elements are effectively equal
    # This handles floating-point precision issues where values are extremely close
    # but not bitwise identical
    tolerance = 1e-10
    all_equal = all(math.isclose(x, data[0], abs_tol=tolerance) for x in data)
    variance_is_zero = math.isclose(result, 0, abs_tol=tolerance)
    
    assert all_equal == variance_is_zero

@given(st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10).filter(lambda x: abs(x) > 1e-100 or x == 0), min_size=2, max_size=50))
def test_stdev_equals_sqrt_of_variance_floats(data):
    """Test that stdev(data) == sqrt(variance(data)) for floating point data.
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance: stdev is the square root of variance.
    
    The strategy is constrained to reasonable floating-point ranges (-1e10 to 1e10)
    and filters out extremely small numbers (those with absolute value between 0 and 1e-100)
    to avoid precision issues with floating-point arithmetic that can cause
    the square root calculation to differ from direct standard deviation computation
    due to accumulated rounding errors.
    """
    import math
    
    stdev_result = stdev(data)
    variance_result = variance(data)
    sqrt_variance = math.sqrt(variance_result)
    
    # Use a more lenient tolerance that accounts for floating-point precision
    # issues, especially when dealing with very small numbers or when rounding
    # errors accumulate differently in the two calculation paths
    assert math.isclose(stdev_result, sqrt_variance, rel_tol=1e-6, abs_tol=1e-15)

@given(st.lists(st.decimals(min_value=-1000, max_value=1000, allow_nan=False, allow_infinity=False), min_size=2, max_size=20))
def test_stdev_equals_sqrt_of_variance_decimals(data):
    """Test that stdev(data) == sqrt(variance(data)) for Decimal data.
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance with Decimal inputs, which should
    maintain high precision. The relationship stdev = sqrt(variance) 
    should hold for all valid datasets, including edge cases where 
    variance is zero.
    """
    from decimal import Decimal, getcontext
    
    # Set high precision for Decimal calculations
    original_precision = getcontext().prec
    getcontext().prec = 50
    
    try:
        stdev_result = stdev(data)
        variance_result = variance(data)
        
        # The fundamental relationship should always hold: stdev = sqrt(variance)
        if variance_result == Decimal('0'):
            # When variance is zero, standard deviation must also be zero
            assert stdev_result == Decimal('0'), f"Expected stdev=0 when variance=0, got stdev={stdev_result}"
        else:
            # For non-zero variance, test the fundamental relationship
            sqrt_variance = variance_result.sqrt()
            
            # Use relative tolerance for high-precision Decimal comparison
            # This accounts for potential minor precision differences in calculations
            if sqrt_variance != Decimal('0'):
                relative_diff = abs(stdev_result - sqrt_variance) / abs(sqrt_variance)
                tolerance = Decimal('1e-25')  # Very tight tolerance for Decimals
                assert relative_diff <= tolerance, f"stdev={stdev_result} != sqrt(variance)={sqrt_variance}, relative_diff={relative_diff} > tolerance={tolerance}"
            else:
                # If sqrt_variance is exactly zero, stdev should also be exactly zero
                assert stdev_result == Decimal('0'), f"Expected stdev=0 when sqrt(variance)=0, got stdev={stdev_result}"
    
    finally:
        # Restore original precision
        getcontext().prec = original_precision

@given(
    st.lists(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10), min_size=2, max_size=50),
    st.floats(allow_nan=False, allow_infinity=False, min_value=-1e10, max_value=1e10)
)
def test_stdev_equals_sqrt_of_variance_with_xbar(data, xbar):
    """Test that stdev(data, xbar) == sqrt(variance(data, xbar)) when xbar is provided.
    
    This property tests the mathematical relationship holds even when
    an explicit mean (xbar) is provided to both functions.
    
    The floating-point values are restricted to a reasonable range to avoid
    numerical instability that occurs with extremely large values where
    variance calculations can exceed floating-point precision limits.
    """
    import math
    
    stdev_result = stdev(data, xbar)
    variance_result = variance(data, xbar)
    
    # Handle potential negative variance due to floating-point errors
    if variance_result < 0:
        # If variance is slightly negative due to floating-point errors, treat as zero
        if abs(variance_result) < 1e-14:
            sqrt_variance = 0.0
        else:
            # If significantly negative, this indicates a real problem
            raise AssertionError(f"Variance should not be negative: {variance_result}")
    else:
        sqrt_variance = math.sqrt(variance_result)
    
    # Use relative tolerance with a reasonable minimum absolute tolerance
    # The absolute tolerance should be proportional to the expected magnitude
    expected_magnitude = max(abs(stdev_result), abs(sqrt_variance))
    abs_tolerance = max(1e-14, expected_magnitude * 1e-12)
    
    assert math.isclose(stdev_result, sqrt_variance, rel_tol=1e-9, abs_tol=abs_tolerance)

@given(st.lists(st.fractions(min_value=-100, max_value=100), min_size=2, max_size=20))
def test_stdev_equals_sqrt_of_variance_fractions(data):
    """Test that stdev(data) == sqrt(variance(data)) for Fraction data.
    
    This property tests the fundamental mathematical relationship between
    standard deviation and variance with Fraction inputs. Special handling
    is needed for the case where variance is 0 (all values identical) and
    for maintaining precision when working with Fraction arithmetic.
    
    To avoid precision issues with sqrt calculations on Fractions, we test
    the equivalent property: stdev² == variance, which allows exact comparison.
    """
    from fractions import Fraction
    
    stdev_result = stdev(data)
    variance_result = variance(data)
    
    # Verify that results are Fractions when input is Fractions
    assert isinstance(stdev_result, Fraction), f"Expected Fraction, got {type(stdev_result)}"
    assert isinstance(variance_result, Fraction), f"Expected Fraction, got {type(variance_result)}"
    
    # Handle the case where variance is 0 (all values are the same)
    if variance_result == 0:
        # When variance is 0, standard deviation must also be 0
        assert stdev_result == 0
    else:
        # For the mathematical relationship stdev = sqrt(variance), we test
        # the equivalent property stdev² = variance to avoid sqrt precision issues
        stdev_squared = stdev_result * stdev_result
        
        # Direct comparison of Fractions should be exact
        assert stdev_squared == variance_result, \
            f"stdev²({stdev_squared}) != variance({variance_result})"

import math

@given(st.lists(st.integers(min_value=-1000, max_value=1000), min_size=2, max_size=50))
def test_variance_with_explicit_mean_equals_variance_with_none_integers(data):
    """
    Test that variance calculations with explicit mean and auto-calculated mean 
    produce equivalent results.
    
    Even with integer inputs, variance calculations involve division operations
    that can produce floating point results, so we use math.isclose() for
    appropriate floating point comparison rather than exact equality.
    """
    # Calculate the mean of the data
    computed_mean = mean(data)
    
    # Get variance with explicit mean
    variance_with_mean = variance(data, computed_mean)
    
    # Get variance with None (auto-calculated mean)
    variance_with_none = variance(data, None)
    
    # Use floating point comparison with reasonable tolerance since variance 
    # calculations can produce floating point results even with integer inputs.
    # The tolerance is set to 1e-9 to allow for minor numerical differences
    # while still catching actual bugs.
    assert math.isclose(variance_with_mean, variance_with_none, rel_tol=1e-9)

import math

@given(st.lists(st.integers(-1000, 1000), min_size=2, max_size=50))
def test_stdev_with_explicit_mean_equals_stdev_with_none_integers(data):
    """
    Test that stdev(data, mean(data)) ≈ stdev(data, None) for integer data.
    
    This tests that calculating standard deviation with an explicitly provided mean
    produces approximately the same result as letting stdev calculate the mean internally.
    The results should be very close but may differ slightly due to floating-point
    precision differences in how the mean is calculated and used.
    """
    from statistics import stdev, mean
    
    # Calculate stdev with explicit mean
    data_mean = mean(data)
    stdev_with_mean = stdev(data, data_mean)
    
    # Calculate stdev with None (let it calculate mean internally)
    stdev_with_none = stdev(data, None)
    
    # Use approximate equality to account for floating-point precision issues
    # Using rel_tol=1e-12 to be more tolerant of legitimate floating-point differences
    assert math.isclose(stdev_with_mean, stdev_with_none, rel_tol=1e-12), \
        f"stdev with explicit mean {stdev_with_mean} != stdev with None {stdev_with_none}"

import pytest

@given(st.one_of(
    st.integers(),
    st.floats(allow_nan=False, allow_infinity=False),
    st.fractions(),
    st.decimals(allow_nan=False, allow_infinity=False),
    st.text(min_size=1),
    st.booleans()
))
def test_single_element_central_tendency_equality(x):
    """
    Test that for single-element data [x], mean([x]) == median([x]) == mode([x]) == x.
    
    This property should hold for any single value regardless of type, as all three
    measures of central tendency collapse to the single value when there's only one
    data point.
    
    Note: Some string values may cause StatisticsError in statistics.mode(), 
    which we handle by skipping those test cases since they represent edge cases
    where the statistics module cannot process the input.
    """
    from statistics import mean, median, mode, StatisticsError
    import math
    
    single_element_data = [x]
    
    try:
        mean_result = mean(single_element_data)
        median_result = median(single_element_data)
        mode_result = mode(single_element_data)
        
        # For numeric types, we need to handle floating point comparison carefully
        if isinstance(x, float):
            # All results should be close to x and to each other
            assert math.isclose(mean_result, x, rel_tol=1e-15)
            assert math.isclose(median_result, x, rel_tol=1e-15)
            assert mode_result == x  # mode should return exact value
            assert math.isclose(mean_result, median_result, rel_tol=1e-15)
            assert math.isclose(mean_result, mode_result, rel_tol=1e-15)
        else:
            # For non-float types, exact equality should hold
            assert mean_result == x
            assert median_result == x
            assert mode_result == x
            assert mean_result == median_result == mode_result
            
    except StatisticsError:
        # Skip this test case if statistics functions can't handle the input
        # This can happen with certain string values that mode() cannot process
        pytest.skip(f"Statistics functions cannot handle input: {repr(x)}")

@given(st.one_of(
    st.integers(min_value=-(2**53-1), max_value=2**53-1),
    st.floats(allow_nan=False, allow_infinity=False)
), st.integers(min_value=1, max_value=100))
def test_constant_numeric_data_central_tendency_equality(constant_value, list_length):
    """
    Test that for constant numeric data [x, x, ..., x], all measures of central tendency
    (mean, median, mode) equal the constant value x.
    
    This property should hold for any constant numeric value (integers and floats)
    and any positive list length. Integers are limited to the safe floating-point
    range (-(2^53-1) to 2^53-1) to avoid precision issues when statistics functions
    internally use floating-point arithmetic.
    """
    from statistics import mean, median, mode
    import math
    
    # Create constant data list
    constant_data = [constant_value] * list_length
    
    # Calculate all measures of central tendency
    calculated_mean = mean(constant_data)
    calculated_median = median(constant_data)
    calculated_mode = mode(constant_data)
    
    # For floating point numbers, use approximate equality due to potential precision issues
    if isinstance(constant_value, float):
        assert math.isclose(calculated_mean, constant_value), f"Mean {calculated_mean} != {constant_value}"
        assert math.isclose(calculated_median, constant_value), f"Median {calculated_median} != {constant_value}"
        assert math.isclose(calculated_mode, constant_value), f"Mode {calculated_mode} != {constant_value}"
        
        # Verify that all three measures are equal to each other
        assert math.isclose(calculated_mean, calculated_median), f"Mean {calculated_mean} != Median {calculated_median}"
        assert math.isclose(calculated_median, calculated_mode), f"Median {calculated_median} != Mode {calculated_mode}"
        assert math.isclose(calculated_mean, calculated_mode), f"Mean {calculated_mean} != Mode {calculated_mode}"
    else:
        # For exact types (int), use exact equality
        assert calculated_mean == constant_value, f"Mean {calculated_mean} != {constant_value}"
        assert calculated_median == constant_value, f"Median {calculated_median} != {constant_value}"
        assert calculated_mode == constant_value, f"Mode {calculated_mode} != {constant_value}"
        
        # Verify that all three measures are equal to each other
        assert calculated_mean == calculated_median == calculated_mode, f"Not all equal: mean={calculated_mean}, median={calculated_median}, mode={calculated_mode}"

@given(st.one_of(
    st.text(),
    st.booleans()
), st.integers(min_value=1, max_value=100))
def test_constant_non_numeric_data_central_tendency_equality(constant_value, list_length):
    """
    Test that for constant non-numeric data [x, x, ..., x], mode
    equals the constant value x. Median is only tested for odd-length lists
    since it's not well-defined for non-numeric data with even length.
    
    This property should hold for any constant non-numeric value (strings and booleans)
    and any positive list length. For even-length lists of non-numeric data,
    median() attempts to average the middle elements which is undefined for non-numeric types.
    """
    from statistics import median, mode
    
    # Create constant data list
    constant_data = [constant_value] * list_length
    
    # Mode should always equal the constant value for constant data
    calculated_mode = mode(constant_data)
    assert calculated_mode == constant_value, f"Mode {calculated_mode} != {constant_value}"
    
    # Median only makes sense for non-numeric data when list length is odd
    # For even-length lists, median() tries to average middle elements which fails for non-numeric data
    if list_length % 2 == 1:
        calculated_median = median(constant_data)
        assert calculated_median == constant_value, f"Median {calculated_median} != {constant_value}"
        assert calculated_median == calculated_mode, f"Median {calculated_median} != Mode {calculated_mode}"